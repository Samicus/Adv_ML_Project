{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for loading data, setting up a model, running training and making predictions\n",
    "\n",
    "Copied from `testEDA_pontus.ipynb`, only including the relevant parts for the project solution. Does not include tests or unused functions.\n",
    "Also made some parts more generalized, for example making it possible to run the model for arbitrary batch size.\n",
    "\n",
    "Implement new ideas and tweak parameters using this notebook, try to improve the results. Save checkpoints/good models as separate notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index                                                                  0\nid                                                          id_001f94081\nsequence               GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...\nstructure              .....((((((.......)))).)).((.....((..((((((......\npredicted_loop_type    EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...\nsignal_to_noise                                                    6.894\nSN_filter                                                              1\nseq_length                                                           107\nseq_scored                                                            68\nreactivity_error       [0.1359, 0.20700000000000002, 0.1633, 0.1452, ...\ndeg_error_Mg_pH10      [0.26130000000000003, 0.38420000000000004, 0.1...\ndeg_error_pH10         [0.2631, 0.28600000000000003, 0.0964, 0.1574, ...\ndeg_error_Mg_50C       [0.1501, 0.275, 0.0947, 0.18660000000000002, 0...\ndeg_error_50C          [0.2167, 0.34750000000000003, 0.188, 0.2124, 0...\nreactivity             [0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...\ndeg_Mg_pH10            [0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...\ndeg_pH10               [2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...\ndeg_Mg_50C             [0.35810000000000003, 2.9683, 0.2589, 1.4552, ...\ndeg_50C                [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train = pd.read_json('data/train.json', lines=True) \n",
    "test = pd.read_json('data/test.json', lines=True) \n",
    "# Divide test data into the two subsets: Private Test and Public Test\n",
    "# seq_length=107 in Public Test while seq_length=130 in Private Test\n",
    "test_public = test[test[\"seq_length\"] == 107]\n",
    "test_private = test[test[\"seq_length\"] == 130]\n",
    "\n",
    "# Print the first sample for testing\n",
    "df = pd.DataFrame(train)\n",
    "print(df.iloc[0])\n",
    "\n",
    "# Optionally, only take training data which have passed the signal-to-noise filter\n",
    "train_filtered = train[train[\"SN_filter\"] == 1]\n",
    "\n",
    "# Change apply_SN_filter to True to only train on filtered data, using the SN filter described in the Kaggle challenge (same which is used for public test data)\n",
    "apply_SN_filter = True\n",
    "# apply_SN_filter = False \n",
    "if apply_SN_filter == True:\n",
    "    train = train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struct_adj(data = train, sequential_edges = False):\n",
    "    # Get adjacency matrix from sample structure sequence\n",
    "    # Include edges between base pairs\n",
    "    # If sequential_edges == False, do not include edges between sequential bases\n",
    "    # If sequential_edges == True, add these edges, which correspond to the diagonals -1 and 1 in the adjacency matrix (assuming undirected edges)\n",
    "    struct_adj = []\n",
    "    for ix in range(len(data)):\n",
    "        seq_length = data[\"seq_length\"].iloc[ix]\n",
    "        structure = data[\"structure\"].iloc[ix]\n",
    "        sequence = data[\"sequence\"].iloc[ix]\n",
    "\n",
    "        queue = [] # Store indices corresponding to \"(\" in queue\n",
    "\n",
    "        sample_struct_adj = np.zeros([seq_length, seq_length])\n",
    "        for jx in range(seq_length):\n",
    "            if structure[jx] == \"(\":\n",
    "                queue.append(jx) # Append index of \"(\" in base pair to queue\n",
    "            elif structure[jx] == \")\":\n",
    "                start = queue.pop() # Retrieve index of last \"(\" in queue, corresponding to \")\" at jx\n",
    "                sample_struct_adj[start, jx] = 1 # Add edge from \"(\" to \")\"\n",
    "                sample_struct_adj[jx, start] = 1 # Add edge from \")\" to \"(\" (assume undirected)\n",
    "\n",
    "        if sequential_edges == True:\n",
    "            ones = np.ones(seq_length-1) # Match length of -1 and 1 diagonals in sample_struct_adj\n",
    "            sample_struct_adj += np.diag(ones,1) # Add sequential edges (i,i+1) \n",
    "            sample_struct_adj += np.diag(ones,-1) # Add sequential edges (i+1,i) (assume non-directed)\n",
    "\n",
    "        struct_adj.append(sample_struct_adj)\n",
    "\n",
    "    struct_adj = np.array(struct_adj)\n",
    "    return struct_adj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for constructing distance adjacency matrix\n",
    "# Only returns one distance adjacency matrix, since it is identical for all samples (only depends on number of nodes)\n",
    "def get_dist_adj(data = train, power = 1):\n",
    "    # Get adjacency matrix from inverse index-based distance between nodes\n",
    "    # power is the variable p in the expression D(i,j)\n",
    "    dist_adj = []\n",
    "    idx = np.arange(data[\"seq_length\"].iloc[0]) # Get number of nodes\n",
    "    for ix in range(len(idx)):\n",
    "        d = np.abs(idx[ix] - idx) # Get distance from individual nodes to all other nodes\n",
    "        dist_adj.append(d)\n",
    "\n",
    "    # Convert distance to distance measure according to formula    \n",
    "    dist_adj = np.array(dist_adj) + 1 # Add one to avoid singularity at d=0\n",
    "    dist_adj = 1/dist_adj # Inverse of distance\n",
    "    dist_adj = dist_adj**power # Apply the specified power\n",
    "    return dist_adj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base pair probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1589 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78ee3211d0b0407cb073f44d4887faa3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/629 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9b6a391a61741bf9e7f84853fd30079"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Load the provided base pair probability adjacency matrices for the samples included in the datasets\n",
    "\n",
    "# Train\n",
    "Adj_bpps = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps.append(bpps)\n",
    "Adj_bpps = np.array(Adj_bpps)\n",
    "\n",
    "# Public test\n",
    "Adj_bpps_test_public = []\n",
    "for id in tqdm(test_public[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps_test_public.append(bpps)\n",
    "Adj_bpps_test_public = np.array(Adj_bpps_test_public)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(data = train):\n",
    "    # Create a node feature matrix for each sample in data\n",
    "    # Encode feature vectors as one-hot arrays  \n",
    "    # Included features: \n",
    "    #   Base (given by sequence)\n",
    "    #   Loop type (given by predicted_loop_type)\n",
    "    # Could also include sequence, i.e. \".\" \"(\" and \")\", but I don't see how this provides any interesting information if the structure adjacency matrix is used\n",
    "    X = [] # Stacked node feature matrices for all samples in data\n",
    "    \n",
    "    for ix in range(len(data)):\n",
    "        seq_length = data[\"seq_length\"].iloc[ix]\n",
    "        sequence = data[\"sequence\"].iloc[ix]\n",
    "        predicted_loop_type = data[\"predicted_loop_type\"].iloc[ix]\n",
    "\n",
    "        X_sample = [] # Node feature matrix for current sample\n",
    "\n",
    "        for jx in range(seq_length):\n",
    "            # Base one hot\n",
    "            bases = np.array(['A', 'G', 'U', 'C']) # Different order than reference notebook (A,G,C,U)\n",
    "            x_base = np.zeros(len(bases))\n",
    "            x_base[bases == sequence[jx]] = 1 # Set base one-hot to 1 at correct index\n",
    "\n",
    "            # Predicted Loop Type one hot\n",
    "            loop_types = np.array(['S', 'M', 'I', 'B', 'H', 'E', 'X'])\n",
    "            x_loop = np.zeros(len(loop_types))\n",
    "            x_loop[loop_types == predicted_loop_type[jx]] = 1 # Set loop-type one-hot to 1 at correct index\n",
    "\n",
    "            x = np.concatenate((x_base,x_loop)) # Concatenate to one node feature vector\n",
    "            X_sample.append(x) # Append node feature vector to node feature matrix\n",
    "        X_sample = np.array(X_sample)\n",
    "        X.append(X_sample) # Append node feature matrix for current graph\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct node features and adjacency matrix for training data\n",
    "Currently includes Structure and Distance adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Train\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (1589, 107, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (1589, 107, 107, 3)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for training data\n",
    "print(\"Shapes of inputs - Train\")\n",
    "\n",
    "# Node features\n",
    "X = get_node_features(data = train)\n",
    "X = X.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs = get_struct_adj(data = train)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs.shape)\n",
    "# Distance adjacency\n",
    "Adj_dist = get_dist_adj(data = train, power = 1)\n",
    "Adj_dist = Adj_dist[None, :,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist = np.repeat(Adj_dist, len(train), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj = np.concatenate([Adj_pairs[:,:,:,None], Adj_dist[:,:,:,None], Adj_bpps[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj = Adj.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and pretrain denoise model and encode targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1095, 340)\n",
      "69\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "import torch\n",
    "\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\n",
    "error_labels = [\"reactivity_error\", \"deg_error_Mg_pH10\", \"deg_error_Mg_50C\", \"deg_error_pH10\", \"deg_error_50C\"]\n",
    "\n",
    "train_ae = train[train.signal_to_noise > 4].reset_index(drop = True) # remove noisy data\n",
    "y_train_ae = []\n",
    "\n",
    "# Construct target labels\n",
    "for target in target_labels:\n",
    "    y_ae = np.vstack(train_ae[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_train_ae.append(y_ae) # Append array for each target\n",
    "y_train_ae = np.stack(y_train_ae, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_train_ae = y_train_ae.reshape(y_train_ae.shape[0],-1).astype(float)\n",
    "print(y_train_ae.shape)\n",
    "\n",
    "# Construct error labels\n",
    "y_error_ae = []\n",
    "for label in error_labels:\n",
    "    y_ae = np.vstack(train_ae[label]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_error_ae.append(y_ae) # Append array for each target\n",
    "y_error_ae = np.stack(y_error_ae, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_error_ae = y_error_ae.reshape(y_error_ae.shape[0],-1).astype(float) # flatten and cast to float\n",
    "\n",
    "\n",
    "y_train_loader = DataLoader(y_train_ae, batch_size=16, shuffle=True)\n",
    "y_error_loader = DataLoader(y_error_ae, batch_size=16, shuffle=True)\n",
    "print(len(y_train_loader))\n",
    "print(len(y_error_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class AE(nn.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(AE,self).__init__()\n",
    "    torch.manual_seed(12345) # For reproducible results\n",
    "\n",
    "    self.encoder=nn.Sequential(\n",
    "                  nn.Linear(kwargs[\"input_shape\"],512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,512),\n",
    "                  #nn.ReLU(True)\n",
    "        \n",
    "                  )\n",
    "    \n",
    "    self.decoder=nn.Sequential(\n",
    "                  nn.Linear(512,512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,kwargs[\"input_shape\"]),\n",
    "                  )\n",
    "    \n",
    " \n",
    "  def forward(self,x):\n",
    "    x = F.dropout(x, p=0.4, training=True)\n",
    "    x=self.encoder(x)\n",
    "    x=self.decoder(x)\n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2,        101] loss: 0.04828\n",
      "[3,        201] loss: 0.09023\n",
      "[5,        301] loss: 0.03047\n",
      "[6,        401] loss: 0.06530\n",
      "[8,        501] loss: 0.01918\n",
      "[9,        601] loss: 0.05013\n",
      "[11,        701] loss: 0.00961\n",
      "[12,        801] loss: 0.03922\n",
      "[14,        901] loss: 0.00250\n",
      "[15,       1001] loss: 0.03078\n",
      "[16,       1101] loss: 0.05758\n",
      "[18,       1201] loss: 0.02269\n",
      "[19,       1301] loss: 0.04852\n",
      "[21,       1401] loss: 0.01538\n",
      "[22,       1501] loss: 0.04043\n",
      "[24,       1601] loss: 0.01002\n",
      "[25,       1701] loss: 0.03314\n",
      "[27,       1801] loss: 0.00405\n",
      "[28,       1901] loss: 0.02701\n",
      "[29,       2001] loss: 0.05004\n",
      "[31,       2101] loss: 0.02150\n",
      "[32,       2201] loss: 0.04356\n",
      "[34,       2301] loss: 0.01515\n",
      "[35,       2401] loss: 0.03650\n",
      "[37,       2501] loss: 0.01073\n",
      "[38,       2601] loss: 0.03076\n",
      "[40,       2701] loss: 0.00583\n",
      "[41,       2801] loss: 0.02503\n",
      "[43,       2901] loss: 0.00127\n",
      "[44,       3001] loss: 0.02078\n",
      "[45,       3101] loss: 0.03868\n",
      "[47,       3201] loss: 0.01505\n",
      "[48,       3301] loss: 0.03459\n",
      "[50,       3401] loss: 0.01105\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Train AE_model\n",
    "\n",
    "autoencoder = AE(input_shape=340)\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "EPOCHS = 50\n",
    "i = 0\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data, label_errors in zip(y_train_loader, y_error_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        \n",
    "        \n",
    "        #construct random tensor [-1,1]\n",
    "        rand_array = torch.rand(size=(label_errors.shape[0], label_errors.shape[1]))*4-2 \n",
    "        \n",
    "        label_errors = torch.mul(label_errors.float(), rand_array.float())\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        x = data + label_errors\n",
    "        # forward + backward + optimize\n",
    "        outputs = autoencoder(x.float())\n",
    "        loss = mse(data.float(), outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        i = i + 1\n",
    "        if i % 100 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %10d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of targets:  (1589, 68, 5)\n"
     ]
    }
   ],
   "source": [
    "# Construct target arrays for training data\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\n",
    "\n",
    "y_train = []\n",
    "seq_length = train[\"seq_length\"].iloc[0] # Get number of nodes (lenght of sequence)\n",
    "seq_scored = train[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "save_index = []\n",
    "for i in range(len(train)):\n",
    "    if float(train[\"signal_to_noise\"].iloc[i]) <= 1:\n",
    "           save_index.append(i)\n",
    "for target in target_labels:\n",
    "    y = np.vstack(train[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_train.append(y) # Append array for each target\n",
    "y_train = np.stack(y_train, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_train = y_train.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Shape of targets: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1589, 68, 5)\n",
      "(1589, 68, 5)\n"
     ]
    }
   ],
   "source": [
    "autoencoder.eval()\n",
    "print(y_train.shape)\n",
    "for i in save_index:\n",
    "    y_train_tensor = torch.Tensor(y_train[i,:,:].astype(float).flatten())\n",
    "    \n",
    "    outputs = autoencoder(y_train_tensor)   \n",
    "    outputs = outputs.reshape(68,5)\n",
    "    y_train[i,:,:] = outputs.detach().numpy() \n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGraphConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The graph neural network operator from the “Weisfeiler and Leman Go \n",
    "    Neural: Higher-order Graph Neural Networks” paper\n",
    "\n",
    "    x' = x_i W_1.T + (Adj x_i) W_2.T\n",
    "\n",
    "    Arguments:\n",
    "        in_channels (int): Number of features (size) of each input node\n",
    "        out_channels (int): Number of features (size) of each output node\n",
    "        n_edge_features (int): Number of edge features, i.e. Adj.shape[-1]\n",
    "    \n",
    "    forward performs the graph neural network operation\n",
    "    Arguments:\n",
    "        x (torch tensor): The input node features of shape (n_samples, n_nodes, in_channels) \n",
    "        Adj (torch tensor): The adjacency matrix of the graph of shape (n_samples, n_nodes, n_nodes, n_edge_features)\n",
    "    Returns: \n",
    "        x' (torch tensor): Output node feature matrix of shape (n_samples, n_nodes, out_channels)\n",
    "    \"\"\"\n",
    "    # Notes:\n",
    "    ## Also should add boolean argument bias for the linear weights.\n",
    "    ## GraphConv seems to only use bias for W_2 if I understand the source code correctly\n",
    "    ## (see lin_r = ... bias=False).a\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, n_edge_features):\n",
    "        super(myGraphConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.n_edge_features = n_edge_features # Get number of edge features (number of stacked adjacency matrices)\n",
    "\n",
    "        self.lin_self = Linear(in_channels, out_channels, bias=True) # bias=False to match GraphConv? Check source code\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1 = Linear(in_channels, out_channels, bias=True)  \n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 4:\n",
    "            raise ValueError(\"Number of edge features can not be larger than 3\") # \"Hard code\" up to 3 edge features\n",
    "\n",
    "        self.reset_parameters()\n",
    "   \n",
    "    def reset_parameters(self):\n",
    "        self.lin_self.reset_parameters()\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1.reset_parameters()\n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2.reset_parameters()\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3.reset_parameters()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # Shapes of arguments, weight matrices and output\n",
    "        # x: (n_samples, n_nodes, in_channels) \n",
    "        # Adj: (n_samples, n_nodes, n_nodes, n_edge_features)\n",
    "        # W_1: (in_channels, out_channels)\n",
    "        # W_2: (in_channels, out_channels)\n",
    "        # out: (n_samples, n_nodes, out_channels)\n",
    "\n",
    "        # Confirm that the input variable n_edge_features matches the adjacency matrix\n",
    "        if self.n_edge_features != Adj.shape[-1]:\n",
    "            raise ValueError(\"Specified number of edge features must match last dimensino in adjacency matrix\") \n",
    "\n",
    "        # Calculate contribution from self (node)\n",
    "        out = self.lin_self(x)\n",
    "\n",
    "        # Add contributions from edges\n",
    "        # Calculate contributions from adjacent nodes\n",
    "        # Use separate weights for each edge feature\n",
    "        if self.n_edge_features >= 1:\n",
    "            out_1 = torch.matmul(Adj[..., 0], x) # This is equivalent to summing over edge weights assuming Adj contains the edge weights\n",
    "            out_1 = self.lin_1(out_1) # Multiply with weight matrix for adjacent nodes\n",
    "            out += out_1 # Add contribution from first edge feature\n",
    "        # Repeat for all edge weights\n",
    "        if self.n_edge_features >= 2:\n",
    "            out_2 = torch.matmul(Adj[..., 1], x) \n",
    "            out_2 = self.lin_2(out_2) \n",
    "            out += out_2 # Add contribution from second edge feature\n",
    "        if self.n_edge_features >= 3:\n",
    "            out_3 = torch.matmul(Adj[..., 2], x)\n",
    "            out_3 = self.lin_3(out_3) \n",
    "            out += out_3 # Add contribution from third edge feature\n",
    "        return out\n",
    "\n",
    "    # The method that returns a printable representation of the operator, copy to match GraphConv source code \n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MCRMSE loss function\n",
    "# Include all 5 targets by default, allow optional argument to calculate MCRMSE of scored targets only.\n",
    "# Assumes targets are ordered such that the first 3 targets are the scored ones.\n",
    "# Inputs should have dimensions (n_samples, n_nodes, n_targets)\n",
    "def MCRMSE(y_true, y_pred, only_scored=False, data = train):\n",
    "    # Reshape if input only includes one sample and has dimensions (n_nodes, n_targets)\n",
    "    if y_true.dim() == 2:\n",
    "        y_true = y_true[None, :, :]\n",
    "    if y_pred.dim() == 2:\n",
    "        y_pred = y_pred[None, :, :]\n",
    "\n",
    "    # Extract the scored targets\n",
    "    seq_scored = data[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "    y_pred = y_pred[:, :seq_scored, :] \n",
    "    # true = y_true[:, :seq_scored, :] # Not necessary since only scored targets are included, could include dummy values instead as in reference notebook\n",
    "\n",
    "    y_diff = y_pred - y_true\n",
    "    mse = torch.mean(y_diff**2, axis=1) # Average over nodes in each sample for every target\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    num_scored = 5 # Include all targets by default\n",
    "    if only_scored == True:\n",
    "        num_scored = 3 # Include only scored targets if specified by keyword (assumes correct ordering of targets in y_true and y_pred)\n",
    "\n",
    "    mcrmse = torch.mean(rmse[:, :num_scored], axis=1) # Average over included targets\n",
    "\n",
    "    return mcrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, n_edge_features, n_node_features = 11):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345) # For reproducible results\n",
    "        self.conv = myGraphConv(n_node_features, hidden_channels, n_edge_features)\n",
    "        self.lin = Linear(hidden_channels, 5) # Map to the 5 output targets with dense layer\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # 1. Obtain node embeddings, use GraphConv layers with ReLU for non-linearity\n",
    "        x = self.conv(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # No pooling is required, we want target labels for each node, not for the entire graph\n",
    "\n",
    "        # 3. Apply a final classifier \n",
    "        # Use a single layer as classifier to map to the targets\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # No LogSoftmax needed, possibly some other function to map to correct targets?\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNN(\n",
      "  (conv): myGraphConv(11, 64)\n",
      "  (lin): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "=== Starting epoch 1 ===\n",
      "[batch     9, sample   144] loss: 0.622\n",
      "[batch    18, sample   288] loss: 0.467\n",
      "[batch    27, sample   432] loss: 0.411\n",
      "[batch    36, sample   576] loss: 0.390\n",
      "[batch    45, sample   720] loss: 0.399\n",
      "[batch    54, sample   864] loss: 0.381\n",
      "[batch    63, sample  1008] loss: 0.380\n",
      "[batch    72, sample  1152] loss: 0.383\n",
      "[batch    81, sample  1296] loss: 0.377\n",
      "[batch    90, sample  1440] loss: 0.382\n",
      "[batch    99, sample  1584] loss: 0.372\n",
      "=== Starting epoch 2 ===\n",
      "[batch     9, sample   144] loss: 0.418\n",
      "[batch    18, sample   288] loss: 0.390\n",
      "[batch    27, sample   432] loss: 0.365\n",
      "[batch    36, sample   576] loss: 0.360\n",
      "[batch    45, sample   720] loss: 0.372\n",
      "[batch    54, sample   864] loss: 0.365\n",
      "[batch    63, sample  1008] loss: 0.357\n",
      "[batch    72, sample  1152] loss: 0.370\n",
      "[batch    81, sample  1296] loss: 0.365\n",
      "[batch    90, sample  1440] loss: 0.372\n",
      "[batch    99, sample  1584] loss: 0.363\n",
      "=== Starting epoch 3 ===\n",
      "[batch     9, sample   144] loss: 0.410\n",
      "[batch    18, sample   288] loss: 0.383\n",
      "[batch    27, sample   432] loss: 0.360\n",
      "[batch    36, sample   576] loss: 0.356\n",
      "[batch    45, sample   720] loss: 0.367\n",
      "[batch    54, sample   864] loss: 0.360\n",
      "[batch    63, sample  1008] loss: 0.353\n",
      "[batch    72, sample  1152] loss: 0.367\n",
      "[batch    81, sample  1296] loss: 0.362\n",
      "[batch    90, sample  1440] loss: 0.368\n",
      "[batch    99, sample  1584] loss: 0.359\n",
      "=== Starting epoch 4 ===\n",
      "[batch     9, sample   144] loss: 0.405\n",
      "[batch    18, sample   288] loss: 0.377\n",
      "[batch    27, sample   432] loss: 0.354\n",
      "[batch    36, sample   576] loss: 0.352\n",
      "[batch    45, sample   720] loss: 0.364\n",
      "[batch    54, sample   864] loss: 0.358\n",
      "[batch    63, sample  1008] loss: 0.350\n",
      "[batch    72, sample  1152] loss: 0.364\n",
      "[batch    81, sample  1296] loss: 0.360\n",
      "[batch    90, sample  1440] loss: 0.366\n",
      "[batch    99, sample  1584] loss: 0.355\n",
      "=== Starting epoch 5 ===\n",
      "[batch     9, sample   144] loss: 0.402\n",
      "[batch    18, sample   288] loss: 0.373\n",
      "[batch    27, sample   432] loss: 0.351\n",
      "[batch    36, sample   576] loss: 0.350\n",
      "[batch    45, sample   720] loss: 0.362\n",
      "[batch    54, sample   864] loss: 0.356\n",
      "[batch    63, sample  1008] loss: 0.347\n",
      "[batch    72, sample  1152] loss: 0.362\n",
      "[batch    81, sample  1296] loss: 0.357\n",
      "[batch    90, sample  1440] loss: 0.362\n",
      "[batch    99, sample  1584] loss: 0.352\n",
      "=== Starting epoch 6 ===\n",
      "[batch     9, sample   144] loss: 0.398\n",
      "[batch    18, sample   288] loss: 0.370\n",
      "[batch    27, sample   432] loss: 0.348\n",
      "[batch    36, sample   576] loss: 0.344\n",
      "[batch    45, sample   720] loss: 0.357\n",
      "[batch    54, sample   864] loss: 0.351\n",
      "[batch    63, sample  1008] loss: 0.344\n",
      "[batch    72, sample  1152] loss: 0.357\n",
      "[batch    81, sample  1296] loss: 0.353\n",
      "[batch    90, sample  1440] loss: 0.358\n",
      "[batch    99, sample  1584] loss: 0.348\n",
      "=== Starting epoch 7 ===\n",
      "[batch     9, sample   144] loss: 0.396\n",
      "[batch    18, sample   288] loss: 0.368\n",
      "[batch    27, sample   432] loss: 0.345\n",
      "[batch    36, sample   576] loss: 0.343\n",
      "[batch    45, sample   720] loss: 0.354\n",
      "[batch    54, sample   864] loss: 0.349\n",
      "[batch    63, sample  1008] loss: 0.341\n",
      "[batch    72, sample  1152] loss: 0.354\n",
      "[batch    81, sample  1296] loss: 0.351\n",
      "[batch    90, sample  1440] loss: 0.355\n",
      "[batch    99, sample  1584] loss: 0.345\n",
      "=== Starting epoch 8 ===\n",
      "[batch     9, sample   144] loss: 0.393\n",
      "[batch    18, sample   288] loss: 0.364\n",
      "[batch    27, sample   432] loss: 0.342\n",
      "[batch    36, sample   576] loss: 0.340\n",
      "[batch    45, sample   720] loss: 0.352\n",
      "[batch    54, sample   864] loss: 0.347\n",
      "[batch    63, sample  1008] loss: 0.339\n",
      "[batch    72, sample  1152] loss: 0.352\n",
      "[batch    81, sample  1296] loss: 0.349\n",
      "[batch    90, sample  1440] loss: 0.353\n",
      "[batch    99, sample  1584] loss: 0.343\n",
      "=== Starting epoch 9 ===\n",
      "[batch     9, sample   144] loss: 0.391\n",
      "[batch    18, sample   288] loss: 0.360\n",
      "[batch    27, sample   432] loss: 0.338\n",
      "[batch    36, sample   576] loss: 0.336\n",
      "[batch    45, sample   720] loss: 0.347\n",
      "[batch    54, sample   864] loss: 0.343\n",
      "[batch    63, sample  1008] loss: 0.337\n",
      "[batch    72, sample  1152] loss: 0.349\n",
      "[batch    81, sample  1296] loss: 0.346\n",
      "[batch    90, sample  1440] loss: 0.350\n",
      "[batch    99, sample  1584] loss: 0.340\n",
      "=== Starting epoch 10 ===\n",
      "[batch     9, sample   144] loss: 0.390\n",
      "[batch    18, sample   288] loss: 0.357\n",
      "[batch    27, sample   432] loss: 0.336\n",
      "[batch    36, sample   576] loss: 0.333\n",
      "[batch    45, sample   720] loss: 0.345\n",
      "[batch    54, sample   864] loss: 0.340\n",
      "[batch    63, sample  1008] loss: 0.335\n",
      "[batch    72, sample  1152] loss: 0.346\n",
      "[batch    81, sample  1296] loss: 0.344\n",
      "[batch    90, sample  1440] loss: 0.349\n",
      "[batch    99, sample  1584] loss: 0.339\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GNN model, optimizer and loss function\n",
    "model = GNN(hidden_channels=64, n_edge_features = Adj.shape[-1])\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Adjust learning rate\n",
    "criterion = MCRMSE # Mean column-wise root mean square error (MCRMSE) loss\n",
    "\n",
    "# Define trainer function for GNN\n",
    "def run_training(X_data, Adj_data, batch_size = 1, n_epochs = 1):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"=== Starting epoch {epoch + 1} ===\")\n",
    "        # Get permutation of sample indices for shuffling\n",
    "        permutation = torch.randperm(len(X_data))\n",
    "        permutation = range(len(X_data))\n",
    "        # Define variable for printing training loss\n",
    "        running_loss = 0.\n",
    "        # Run training over mini-batches for current epoch\n",
    "        for ix in range(0, len(X_data), batch_size):  # Iterate over samples in the training dataset\n",
    "            batch_indices = permutation[ix:ix+batch_size] # Get shuffled indices for minibatch\n",
    "            X_batch, Adj_batch = X_data[batch_indices], Adj_data[batch_indices] # Minibatch of X and Adj\n",
    "            y_batch = y_train[batch_indices] # Minibatch of ground truths\n",
    "            out = model(X_batch, Adj_batch) # Perform forward pass\n",
    "            loss = criterion(torch.tensor(y_batch), out)  # Compute the loss\n",
    "            # As the loss function is defined per sample, we have to reduce the loss for\n",
    "            # each mini-batch to a singular value in some way.\n",
    "            # Could for example use mean, sum, or random sample. This is a design choice.\n",
    "            loss = torch.mean(loss) # Calculate average loss for minibatch\n",
    "            loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            # Print statistics every print_batch minibatches \n",
    "            print_batch = int(len(X_data)/10/batch_size) # Set to print every 1/10 of all samples\n",
    "            running_loss += loss.item() # Add (average) loss from minibatch\n",
    "            if int(ix/batch_size) % print_batch == 0 and ix != 0: # Ignore first minibatch\n",
    "                print('[batch %5d, sample %5d] loss: %.3f' % \n",
    "                        (int(ix/batch_size), ix, \n",
    "                        running_loss / print_batch)) # Average running loss\n",
    "                running_loss = 0. # Reset running loss\n",
    "\n",
    "# Convert training data inputs to pytorch tensors and run training\n",
    "X_torch = torch.tensor(X)\n",
    "Adj_torch = torch.tensor(Adj)\n",
    "run_training(X_torch, Adj_torch, batch_size = 16, n_epochs = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct node features and adjacency matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Public test\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (629, 107, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (629, 107, 107, 3)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for test data\n",
    "print(\"Shapes of inputs - Public test\")\n",
    "\n",
    "# Node features\n",
    "X_test_public = get_node_features(data = test_public)\n",
    "X_test_public = X_test_public.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X_test_public.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs_test_public = get_struct_adj(data = test_public)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs_test_public.shape)\n",
    "# Distance adjacency\n",
    "Adj_dist_test_public = get_dist_adj(data = test_public, power = 1)\n",
    "Adj_dist_test_public = Adj_dist_test_public[None, :,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist_test_public = np.repeat(Adj_dist_test_public, len(test_public), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist_test_public.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps_test_public.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj_test_public = np.concatenate([Adj_pairs_test_public[:,:,:,None], Adj_dist_test_public[:,:,:,None], Adj_bpps_test_public[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj_test_public = Adj_test_public.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj_test_public.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on public test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(629, 107, 5)\n"
     ]
    }
   ],
   "source": [
    "# Define prediction function\n",
    "def run_prediction(X_data, Adj_data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    outs = model(X_data, Adj_data) # Feed the data through the network \n",
    "    for yx in outs:\n",
    "        y_pred.append(yx.detach().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred\n",
    "# Convert test data inputs to pytorch tensors and run prediction\n",
    "X_test_public_torch = torch.tensor(X_test_public)\n",
    "Adj_test_public_torch = torch.tensor(Adj_test_public)\n",
    "y_pred = run_prediction(X_test_public_torch, Adj_test_public_torch)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on training data using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.33994\nMean score on training data, only scored targets: 0.35257\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on private data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Public test\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (2493, 130, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (2493, 130, 130, 3)\n"
     ]
    }
   ],
   "source": [
    "pdeadline_test = pd.read_csv('data/post_deadline_files/private_test_labels.csv')\n",
    "\n",
    "# Private test\n",
    "Adj_bpps_test_private = []\n",
    "for id in pdeadline_test[\"id\"]:\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps_test_private.append(bpps)\n",
    "Adj_bpps_test_private = np.array(Adj_bpps_test_private)\n",
    "\n",
    "# Construct node features and adjacency matrix for test data\n",
    "print(\"Shapes of inputs - Public test\")\n",
    "\n",
    "# Node features\n",
    "X_test_private = get_node_features(data = pdeadline_test)\n",
    "X_test_private = X_test_private.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X_test_private.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs_test_private = get_struct_adj(data = pdeadline_test)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs_test_private.shape)\n",
    "# Distance adjacency\n",
    "# Distance adjacency\n",
    "Adj_dist_priv = get_dist_adj(data = pdeadline_test, power = 1)\n",
    "Adj_dist_priv = Adj_dist_priv[None, :,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist_priv = np.repeat(Adj_dist_priv, len(pdeadline_test), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist_priv.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps_test_public.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj_test_private = np.concatenate([Adj_pairs_test_private[:,:,:,None], Adj_dist_priv[:,:,:,None], Adj_bpps_test_private[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj_test_private = Adj_test_private.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj_test_private.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the private labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-72-c35fbd565040>:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  aa.append(np.fromstring(a[i],dtype=np.float,sep=','))\n<ipython-input-72-c35fbd565040>:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  bb.append(np.fromstring(b[i],dtype=np.float,sep=','))\n<ipython-input-72-c35fbd565040>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  cc.append(np.fromstring(c[i],dtype=np.float,sep=','))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = []#np.zeros(len(pdeadline_test))\n",
    "b = []\n",
    "c = []\n",
    "for i in range(len(pdeadline_test)):\n",
    "    a.append(pdeadline_test['reactivity'][i][1:-1])\n",
    "    b.append(pdeadline_test['deg_Mg_pH10'][i][1:-1])\n",
    "    c.append(pdeadline_test['deg_Mg_50C'][i][1:-1])\n",
    "aa = []\n",
    "bb = []\n",
    "cc = []\n",
    "for i in range(len(pdeadline_test)):\n",
    "    aa.append(np.fromstring(a[i],dtype=np.float,sep=','))\n",
    "    bb.append(np.fromstring(b[i],dtype=np.float,sep=','))\n",
    "    cc.append(np.fromstring(c[i],dtype=np.float,sep=','))\n",
    "    \n",
    "aa = np.array(aa)\n",
    "bb = np.array(bb)\n",
    "cc = np.array(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2493, 102)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True values 3 by len(pdeadline_test) reactivity, degMgpH10, degMg50C\n",
    "\n",
    "labels = np.array([aa.T,bb.T,cc.T]).T\n",
    "del a, b, c,aa,bb,cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2493, 102, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_private_torch = torch.tensor(X_test_private.astype(np.float32))\n",
    "Adj_test_private_torch = torch.tensor(Adj_test_private.astype(np.float32))\n",
    "\n",
    "y_private_pred = run_prediction(X_test_private_torch, Adj_test_private_torch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The error for all three labels is 0.47289\nIndividual errors\n reactivity: 0.36783\n deg Mg pH10: 0.65468\n deg Mg 50C: 0.48161\n"
     ]
    }
   ],
   "source": [
    "labels_preds = np.zeros((len(pdeadline_test),3))\n",
    "for i in range(len(pdeadline_test)):\n",
    "    for j in range(130):\n",
    "        labels_preds[i] = y_private_pred[i,j,:3]\n",
    "\n",
    "labels_preds = labels_preds.T\n",
    "\n",
    "# How long sequence\n",
    "seq_len = 68\n",
    "y_private_pred_to_score =y_private_pred[:,:seq_len,:3]#.shape\n",
    "diff = (labels[:,:seq_len,:]-y_private_pred_to_score)**2\n",
    "\n",
    "reacErr = diff[:,:,0]\n",
    "pH10Err = diff[:,:,1]\n",
    "deg50CErr = diff[:,:,2]\n",
    "\n",
    "#Err = (((reacErr.mean(axis=1)).mean()+(pH10Err.mean(axis=1)).mean()+deg50CErr.mean(axis=1).mean())**0.5)/3#.mean()\n",
    "Err = MCRMSE(torch.tensor(labels[:,:seq_len,:]), torch.tensor(y_private_pred_to_score), only_scored=True)\n",
    "print(f'The error for all three labels is {torch.mean(Err):.5f}')\n",
    "print(f'Individual errors\\n reactivity: {(((reacErr.mean(axis=1)).mean())**0.5):.5f}\\n deg Mg pH10: {(((pH10Err.mean(axis=1)).mean())**0.5):.5f}\\n deg Mg 50C: {(((deg50CErr.mean(axis=1)).mean())**0.5):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do in this notebook:\n",
    "* Modify GraphConv and GNN so that the adjacency matrix Adj can have shape `[n_samples, n_nodes, n_nodes, n_edge_features]` *Done*\n",
    "* Test the possible ways to include multiple edge features to see which seems to give the best results\n",
    "* Modify training so that one can iterate over mini-batches *Done*\n",
    "* Test if training over mini-batches helps with score\n",
    "\n",
    "New in this notebook/currently implemented:\n",
    "* Concatenation of adjacency matrices into one array with shape `[n_samples, n_nodes, n_nodes, n_edge_features]`\n",
    "* myGraphConv that takes adjacency matrix with shape `[n_samples, n_nodes, n_nodes, n_edge_features]`, currently using separate term and weights for each edge feature, hard coded for up to 3 edge features\n",
    "* Updated MCRMSE so that it can take both single samples and multiple at once, both as shapes [n_nodes,n_targets=5] and [n_samples, n_nodes,n_targets=5]\n",
    "* Updated GNN and myGraphConv so that they can take both single samples and multiple at once\n",
    "* Updated run_prediction to make predictions for all samples in one pass (no iteration required)\n",
    "* Added bpps adjacency matrices, easy to include now with updates to GNN. Scores improved slightly.\n",
    "* Added alternative GraphConv layer implementation, where the outputs of each adjacency matrix*x*W is stacked instead of added together. Added an additional linear layer to map to outputs. This improved the predictions a bit. However, this could be overfitting, since we are using more parameters when stacking the outputs from the GraphConv layer than when summing them. Moved this function to `testEDA_pontus.ipynb` to avoid clutter (myGraphConv_expand function and one line of code in GNN to call the right function).\n",
    "\n",
    "Note that most functions are different than the ones in `testEDA_pontus.ipynb` with the same names, save as new functions if copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd0b6e7aa44a7d439fa282c0ba85474b11133df70cb8e5af25f06f3db395007d18d",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "b6e7aa44a7d439fa282c0ba85474b11133df70cb8e5af25f06f3db395007d18d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}