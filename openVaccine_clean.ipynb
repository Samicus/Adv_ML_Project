{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0b6e7aa44a7d439fa282c0ba85474b11133df70cb8e5af25f06f3db395007d18d",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b6e7aa44a7d439fa282c0ba85474b11133df70cb8e5af25f06f3db395007d18d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Code for loading data, setting up a model, running training and making predictions\n",
    "\n",
    "Copied from `testEDA_pontus.ipynb`, only including the relevant parts for the project solution. Does not include tests or unused functions.\n",
    "Also made some parts more generalized, for example making it possible to run the model for arbitrary batch size.\n",
    "\n",
    "Implement new ideas and tweak parameters using this notebook, try to improve the results. Save checkpoints/good models as separate notebooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index                                                                  0\nid                                                          id_001f94081\nsequence               GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...\nstructure              .....((((((.......)))).)).((.....((..((((((......\npredicted_loop_type    EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...\nsignal_to_noise                                                    6.894\nSN_filter                                                              1\nseq_length                                                           107\nseq_scored                                                            68\nreactivity_error       [0.1359, 0.20700000000000002, 0.1633, 0.1452, ...\ndeg_error_Mg_pH10      [0.26130000000000003, 0.38420000000000004, 0.1...\ndeg_error_pH10         [0.2631, 0.28600000000000003, 0.0964, 0.1574, ...\ndeg_error_Mg_50C       [0.1501, 0.275, 0.0947, 0.18660000000000002, 0...\ndeg_error_50C          [0.2167, 0.34750000000000003, 0.188, 0.2124, 0...\nreactivity             [0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...\ndeg_Mg_pH10            [0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...\ndeg_pH10               [2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...\ndeg_Mg_50C             [0.35810000000000003, 2.9683, 0.2589, 1.4552, ...\ndeg_50C                [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train = pd.read_json('data/train.json', lines=True) \n",
    "test = pd.read_json('data/test.json', lines=True) \n",
    "# Divide test data into the two subsets: Private Test and Public Test\n",
    "# seq_length=107 in Public Test while seq_length=130 in Private Test\n",
    "test_public = test[test[\"seq_length\"] == 107]\n",
    "test_private = test[test[\"seq_length\"] == 130]\n",
    "\n",
    "# Print the first sample for testing\n",
    "df = pd.DataFrame(train)\n",
    "print(df.iloc[0])\n",
    "\n",
    "# Optionally, only take training data which have passed the signal-to-noise filter\n",
    "train_filtered = train[train[\"SN_filter\"] == 1]\n",
    "\n",
    "# Change apply_SN_filter to True to only train on filtered data, using the SN filter described in the Kaggle challenge (same which is used for public test data)\n",
    "apply_SN_filter = True\n",
    "# apply_SN_filter = False \n",
    "if apply_SN_filter == True:\n",
    "    train = train_filtered"
   ]
  },
  {
   "source": [
    "## Structure adjacency matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struct_adj(data = train, sequential_edges = False):\n",
    "    # Get adjacency matrix from sample structure sequence\n",
    "    # Include edges between base pairs\n",
    "    # If sequential_edges == False, do not include edges between sequential bases\n",
    "    # If sequential_edges == True, add these edges, which correspond to the diagonals -1 and 1 in the adjacency matrix (assuming undirected edges)\n",
    "    struct_adj = []\n",
    "    for ix in range(len(data)):\n",
    "        seq_length = data[\"seq_length\"].iloc[ix]\n",
    "        structure = data[\"structure\"].iloc[ix]\n",
    "        sequence = data[\"sequence\"].iloc[ix]\n",
    "\n",
    "        queue = [] # Store indices corresponding to \"(\" in queue\n",
    "\n",
    "        sample_struct_adj = np.zeros([seq_length, seq_length])\n",
    "        for jx in range(seq_length):\n",
    "            if structure[jx] == \"(\":\n",
    "                queue.append(jx) # Append index of \"(\" in base pair to queue\n",
    "            elif structure[jx] == \")\":\n",
    "                start = queue.pop() # Retrieve index of last \"(\" in queue, corresponding to \")\" at jx\n",
    "                sample_struct_adj[start, jx] = 1 # Add edge from \"(\" to \")\"\n",
    "                sample_struct_adj[jx, start] = 1 # Add edge from \")\" to \"(\" (assume undirected)\n",
    "\n",
    "        if sequential_edges == True:\n",
    "            ones = np.ones(seq_length-1) # Match length of -1 and 1 diagonals in sample_struct_adj\n",
    "            sample_struct_adj += np.diag(ones,1) # Add sequential edges (i,i+1) \n",
    "            sample_struct_adj += np.diag(ones,-1) # Add sequential edges (i+1,i) (assume non-directed)\n",
    "\n",
    "        struct_adj.append(sample_struct_adj)\n",
    "\n",
    "    struct_adj = np.array(struct_adj)\n",
    "    return struct_adj "
   ]
  },
  {
   "source": [
    "## Distance adjacency matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for constructing distance adjacency matrix\n",
    "# Only returns one distance adjacency matrix, since it is identical for all samples (only depends on number of nodes)\n",
    "def get_dist_adj(data = train, power = 1):\n",
    "    # Get adjacency matrix from inverse index-based distance between nodes\n",
    "    # power is the variable p in the expression D(i,j)\n",
    "    dist_adj = []\n",
    "    idx = np.arange(data[\"seq_length\"].iloc[0]) # Get number of nodes\n",
    "    for ix in range(len(idx)):\n",
    "        d = np.abs(idx[ix] - idx) # Get distance from individual nodes to all other nodes\n",
    "        dist_adj.append(d)\n",
    "\n",
    "    # Convert distance to distance measure according to formula    \n",
    "    dist_adj = np.array(dist_adj) + 1 # Add one to avoid singularity at d=0\n",
    "    dist_adj = 1/dist_adj # Inverse of distance\n",
    "    dist_adj = dist_adj**power # Apply the specified power\n",
    "    return dist_adj "
   ]
  },
  {
   "source": [
    "## Base pair probabilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1589 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42d9469f8ff34535addc8c313ed7fe4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/629 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "658f1484c3274541a500a3b0f684343e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Load the provided base pair probability adjacency matrices for the samples included in the datasets\n",
    "\n",
    "# Train\n",
    "Adj_bpps = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps.append(bpps)\n",
    "Adj_bpps = np.array(Adj_bpps)\n",
    "\n",
    "# Public test\n",
    "Adj_bpps_test_public = []\n",
    "for id in tqdm(test_public[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps_test_public.append(bpps)\n",
    "Adj_bpps_test_public = np.array(Adj_bpps_test_public)"
   ]
  },
  {
   "source": [
    "## Node features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(data = train):\n",
    "    # Create a node feature matrix for each sample in data\n",
    "    # Encode feature vectors as one-hot arrays  \n",
    "    # Included features: \n",
    "    #   Base (given by sequence)\n",
    "    #   Loop type (given by predicted_loop_type)\n",
    "    # Could also include sequence, i.e. \".\" \"(\" and \")\", but I don't see how this provides any interesting information if the structure adjacency matrix is used\n",
    "    X = [] # Stacked node feature matrices for all samples in data\n",
    "    \n",
    "    for ix in range(len(data)):\n",
    "        seq_length = data[\"seq_length\"].iloc[ix]\n",
    "        sequence = data[\"sequence\"].iloc[ix]\n",
    "        predicted_loop_type = data[\"predicted_loop_type\"].iloc[ix]\n",
    "\n",
    "        X_sample = [] # Node feature matrix for current sample\n",
    "\n",
    "        for jx in range(seq_length):\n",
    "            # Base one hot\n",
    "            bases = np.array(['A', 'G', 'U', 'C']) # Different order than reference notebook (A,G,C,U)\n",
    "            x_base = np.zeros(len(bases))\n",
    "            x_base[bases == sequence[jx]] = 1 # Set base one-hot to 1 at correct index\n",
    "\n",
    "            # Predicted Loop Type one hot\n",
    "            loop_types = np.array(['S', 'M', 'I', 'B', 'H', 'E', 'X'])\n",
    "            x_loop = np.zeros(len(loop_types))\n",
    "            x_loop[loop_types == predicted_loop_type[jx]] = 1 # Set loop-type one-hot to 1 at correct index\n",
    "\n",
    "            x = np.concatenate((x_base,x_loop)) # Concatenate to one node feature vector\n",
    "            X_sample.append(x) # Append node feature vector to node feature matrix\n",
    "        X_sample = np.array(X_sample)\n",
    "        X.append(X_sample) # Append node feature matrix for current graph\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "source": [
    "## Construct node features, adjacency matrix and targets for training data\n",
    "Currently includes Structure and Distance adjacency matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Train\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (1589, 107, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (1589, 107, 107, 3)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for training data\n",
    "print(\"Shapes of inputs - Train\")\n",
    "\n",
    "# Node features\n",
    "X = get_node_features(data = train)\n",
    "X = X.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs = get_struct_adj(data = train)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs.shape)\n",
    "# Distance adjacency\n",
    "Adj_dist = get_dist_adj(data = train, power = 1)\n",
    "Adj_dist = Adj_dist[None, :,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist = np.repeat(Adj_dist, len(train), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj = np.concatenate([Adj_pairs[:,:,:,None], Adj_dist[:,:,:,None], Adj_bpps[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj = Adj.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of targets:  (1589, 68, 5)\n"
     ]
    }
   ],
   "source": [
    "# Construct target arrays for training data\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\n",
    "\n",
    "y_train = []\n",
    "seq_length = train[\"seq_length\"].iloc[0] # Get number of nodes (lenght of sequence)\n",
    "seq_scored = train[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "for target in target_labels:\n",
    "    y = np.vstack(train[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_train.append(y) # Append array for each target\n",
    "y_train = np.stack(y_train, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_train = y_train.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Shape of targets: \", y_train.shape)\n"
   ]
  },
  {
   "source": [
    "## Define graph convolution layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGraphConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The graph neural network operator from the “Weisfeiler and Leman Go \n",
    "    Neural: Higher-order Graph Neural Networks” paper\n",
    "\n",
    "    x' = x_i W_1.T + (Adj x_i) W_2.T\n",
    "\n",
    "    Arguments:\n",
    "        in_channels (int): Number of features (size) of each input node\n",
    "        out_channels (int): Number of features (size) of each output node\n",
    "        n_edge_features (int): Number of edge features, i.e. Adj.shape[-1]\n",
    "    \n",
    "    forward performs the graph neural network operation\n",
    "    Arguments:\n",
    "        x (torch tensor): The input node features of shape (n_samples, n_nodes, in_channels) \n",
    "        Adj (torch tensor): The adjacency matrix of the graph of shape (n_samples, n_nodes, n_nodes, n_edge_features)\n",
    "    Returns: \n",
    "        x' (torch tensor): Output node feature matrix of shape (n_samples, n_nodes, out_channels)\n",
    "    \"\"\"\n",
    "    # Notes:\n",
    "    ## Also should add boolean argument bias for the linear weights.\n",
    "    ## GraphConv seems to only use bias for W_2 if I understand the source code correctly\n",
    "    ## (see lin_r = ... bias=False).a\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, n_edge_features):\n",
    "        super(myGraphConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.n_edge_features = n_edge_features # Get number of edge features (number of stacked adjacency matrices)\n",
    "\n",
    "        self.lin_self = Linear(in_channels, out_channels, bias=True) # bias=False to match GraphConv? Check source code\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1 = Linear(in_channels, out_channels, bias=True)  \n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 4:\n",
    "            raise ValueError(\"Number of edge features can not be larger than 3\") # \"Hard code\" up to 3 edge features\n",
    "\n",
    "        self.reset_parameters()\n",
    "   \n",
    "    def reset_parameters(self):\n",
    "        self.lin_self.reset_parameters()\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1.reset_parameters()\n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2.reset_parameters()\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3.reset_parameters()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # Shapes of arguments, weight matrices and output\n",
    "        # x: (n_samples, n_nodes, in_channels) \n",
    "        # Adj: (n_samples, n_nodes, n_nodes, n_edge_features)\n",
    "        # W_1: (in_channels, out_channels)\n",
    "        # W_2: (in_channels, out_channels)\n",
    "        # out: (n_samples, n_nodes, out_channels)\n",
    "\n",
    "        # Confirm that the input variable n_edge_features matches the adjacency matrix\n",
    "        if self.n_edge_features != Adj.shape[-1]:\n",
    "            raise ValueError(\"Specified number of edge features must match last dimensino in adjacency matrix\") \n",
    "\n",
    "        # Calculate contribution from self (node)\n",
    "        out = self.lin_self(x)\n",
    "\n",
    "        # Add contributions from edges\n",
    "        # Calculate contributions from adjacent nodes\n",
    "        # Use separate weights for each edge feature\n",
    "        if self.n_edge_features >= 1:\n",
    "            out_1 = torch.matmul(Adj[..., 0], x) # This is equivalent to summing over edge weights assuming Adj contains the edge weights\n",
    "            out_1 = self.lin_1(out_1) # Multiply with weight matrix for adjacent nodes\n",
    "            out += out_1 # Add contribution from first edge feature\n",
    "        # Repeat for all edge weights\n",
    "        if self.n_edge_features >= 2:\n",
    "            out_2 = torch.matmul(Adj[..., 1], x) \n",
    "            out_2 = self.lin_2(out_2) \n",
    "            out += out_2 # Add contribution from second edge feature\n",
    "        if self.n_edge_features >= 3:\n",
    "            out_3 = torch.matmul(Adj[..., 2], x)\n",
    "            out_3 = self.lin_3(out_3) \n",
    "            out += out_3 # Add contribution from third edge feature\n",
    "        return out\n",
    "\n",
    "    # The method that returns a printable representation of the operator, copy to match GraphConv source code \n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n"
   ]
  },
  {
   "source": [
    "## Define loss function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MCRMSE loss function\n",
    "# Include all 5 targets by default, allow optional argument to calculate MCRMSE of scored targets only.\n",
    "# Assumes targets are ordered such that the first 3 targets are the scored ones.\n",
    "# Inputs should have dimensions (n_samples, n_nodes, n_targets)\n",
    "def MCRMSE(y_true, y_pred, only_scored=False, data = train):\n",
    "    # Reshape if input only includes one sample and has dimensions (n_nodes, n_targets)\n",
    "    if y_true.dim() == 2:\n",
    "        y_true = y_true[None, :, :]\n",
    "    if y_pred.dim() == 2:\n",
    "        y_pred = y_pred[None, :, :]\n",
    "\n",
    "    # Extract the scored targets\n",
    "    seq_scored = data[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "    y_pred = y_pred[:, :seq_scored, :] \n",
    "    # true = y_true[:, :seq_scored, :] # Not necessary since only scored targets are included, could include dummy values instead as in reference notebook\n",
    "\n",
    "    y_diff = y_pred - y_true\n",
    "    mse = torch.mean(y_diff**2, axis=1) # Average over nodes in each sample for every target\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    num_scored = 5 # Include all targets by default\n",
    "    if only_scored == True:\n",
    "        num_scored = 3 # Include only scored targets if specified by keyword (assumes correct ordering of targets in y_true and y_pred)\n",
    "\n",
    "    mcrmse = torch.mean(rmse[:, :num_scored], axis=1) # Average over included targets\n",
    "\n",
    "    return mcrmse"
   ]
  },
  {
   "source": [
    "## Define model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, n_edge_features, n_node_features = 11):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345) # For reproducible results\n",
    "        self.conv = myGraphConv(n_node_features, hidden_channels, n_edge_features)\n",
    "        self.lin = Linear(hidden_channels, 5) # Map to the 5 output targets with dense layer\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # 1. Obtain node embeddings, use GraphConv layers with ReLU for non-linearity\n",
    "        x = self.conv(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # No pooling is required, we want target labels for each node, not for the entire graph\n",
    "\n",
    "        # 3. Apply a final classifier \n",
    "        # Use a single layer as classifier to map to the targets\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # No LogSoftmax needed, possibly some other function to map to correct targets?\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "source": [
    "## Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNN(\n",
      "  (conv): myGraphConv(11, 64)\n",
      "  (lin): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "[sample   200] loss: 0.446\n",
      "[sample   400] loss: 0.409\n",
      "[sample   600] loss: 0.393\n",
      "[sample   800] loss: 0.390\n",
      "[sample  1000] loss: 0.370\n",
      "[sample  1200] loss: 0.383\n",
      "[sample  1400] loss: 0.383\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GNN model, optimizer and loss function\n",
    "model = GNN(hidden_channels=64, n_edge_features = Adj.shape[-1])\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Adjust learning rate\n",
    "criterion = MCRMSE # Mean column-wise root mean square error (MCRMSE) loss\n",
    "\n",
    "# Define trainer function for GNN\n",
    "def run_training(X_data, Adj_data):\n",
    "    model.train()\n",
    "    running_loss = 0 # For printing training loss\n",
    "    for ix in range(len(X_data)):  # Iterate over samples in the training dataset\n",
    "        out = model(X_data[ix], Adj_data[ix]) # Perform a single forward pass.\n",
    "    # out = model(X_data, Adj_data) # all in one pass, no for loop\n",
    "        loss = criterion(torch.tensor(y_train[ix]), out)  # Compute the loss\n",
    "        loss.backward()  # Derive gradients\n",
    "        optimizer.step()  # Update parameters based on gradients\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if ix % 200 == 199:    # Print average loss every 200 mini-batches (every 200 samples in this case)\n",
    "            print('[sample %5d] loss: %.3f' %\n",
    "                    (ix + 1, running_loss / 200))\n",
    "            running_loss = 0.0 # Reset running loss\n",
    "\n",
    "# Convert training data inputs to pytorch tensors and run training\n",
    "X_torch = torch.tensor(X)\n",
    "Adj_torch = torch.tensor(Adj)\n",
    "run_training(X_torch, Adj_torch)\n"
   ]
  },
  {
   "source": [
    "## Construct node features and adjacency matrix for test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Public test\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (629, 107, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (629, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (629, 107, 107, 3)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for test data\n",
    "print(\"Shapes of inputs - Public test\")\n",
    "\n",
    "# Node features\n",
    "X_test_public = get_node_features(data = test_public)\n",
    "X_test_public = X_test_public.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X_test_public.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs_test_public = get_struct_adj(data = test_public)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs_test_public.shape)\n",
    "# Distance adjacency\n",
    "Adj_dist_test_public = get_dist_adj(data = test_public, power = 1)\n",
    "Adj_dist_test_public = Adj_dist_test_public[None, :,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist_test_public = np.repeat(Adj_dist_test_public, len(test_public), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist_test_public.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps_test_public.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj_test_public = np.concatenate([Adj_pairs_test_public[:,:,:,None], Adj_dist_test_public[:,:,:,None], Adj_bpps_test_public[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj_test_public = Adj_test_public.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj_test_public.shape)"
   ]
  },
  {
   "source": [
    "## Make predictions on public test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(629, 107, 5)\n"
     ]
    }
   ],
   "source": [
    "# Define prediction function\n",
    "def run_prediction(X_data, Adj_data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    outs = model(X_data, Adj_data) # Feed the data through the network \n",
    "    for yx in outs:\n",
    "        y_pred.append(yx.detach().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred\n",
    "# Convert test data inputs to pytorch tensors and run prediction\n",
    "X_test_public_torch = torch.tensor(X_test_public)\n",
    "Adj_test_public_torch = torch.tensor(Adj_test_public)\n",
    "y_pred = run_prediction(X_test_public_torch, Adj_test_public_torch)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "source": [
    "## Make predictions on training data using trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.38034\nMean score on training data, only scored targets: 0.39525\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")"
   ]
  },
  {
   "source": [
    "To do in this notebook:\n",
    "* Modify GraphConv and GNN so that the adjacency matrix Adj can have shape `[n_samples, n_nodes, n_nodes, n_edge_features]` *Done*\n",
    "* Test the possible ways to include multiple edge features to see which seems to give the best results\n",
    "* Modify training so that one can iterate over mini-batches *Done*\n",
    "* Test if training over mini-batches helps with score\n",
    "\n",
    "New in this notebook/currently implemented:\n",
    "* Concatenation of adjacency matrices into one array with shape `[n_samples, n_nodes, n_nodes, n_edge_features]`\n",
    "* myGraphConv that takes adjacency matrix with shape `[n_samples, n_nodes, n_nodes, n_edge_features]`, currently using separate term and weights for each edge feature, hard coded for up to 3 edge features\n",
    "* Updated MCRMSE so that it can take both single samples and multiple at once, both as shapes [n_nodes,n_targets=5] and [n_samples, n_nodes,n_targets=5]\n",
    "* Updated GNN and myGraphConv so that they can take both single samples and multiple at once\n",
    "* Updated run_prediction to make predictions for all samples in one pass (no iteration required)\n",
    "* Added bpps adjacency matrices, easy to include now with updates to GNN. Scores improved slightly.\n",
    "* Added alternative GraphConv layer implementation, where the outputs of each adjacency matrix*x*W is stacked instead of added together. Added an additional linear layer to map to outputs. This improved the predictions a bit. However, this could be overfitting, since we are using more parameters when stacking the outputs from the GraphConv layer than when summing them. Moved this function to `testEDA_pontus.ipynb` to avoid clutter (myGraphConv_expand function and one line of code in GNN to call the right function).\n",
    "\n",
    "Note that most functions are different than the ones in `testEDA_pontus.ipynb` with the same names, save as new functions if copied."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}