{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 mRNA Vaccine Degradation Prediction using Graph Neural Networks\n",
    "## Group 8: Anders Segerlund, Mathias Samuelsson, Pontus Havström, Sam Nehmé\n",
    "\n",
    "This notebook includes the code used to generate the results presented in the report. The data sets were downloaded from https://www.kaggle.com/c/stanford-covid-vaccine/data.\n",
    "\n",
    "The code is not very cleanly written, but should give an insight into how the models were implemented for the curious reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index                                                                  0\nid                                                          id_001f94081\nsequence               GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...\nstructure              .....((((((.......)))).)).((.....((..((((((......\npredicted_loop_type    EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...\nsignal_to_noise                                                    6.894\nSN_filter                                                              1\nseq_length                                                           107\nseq_scored                                                            68\nreactivity_error       [0.1359, 0.20700000000000002, 0.1633, 0.1452, ...\ndeg_error_Mg_pH10      [0.26130000000000003, 0.38420000000000004, 0.1...\ndeg_error_pH10         [0.2631, 0.28600000000000003, 0.0964, 0.1574, ...\ndeg_error_Mg_50C       [0.1501, 0.275, 0.0947, 0.18660000000000002, 0...\ndeg_error_50C          [0.2167, 0.34750000000000003, 0.188, 0.2124, 0...\nreactivity             [0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...\ndeg_Mg_pH10            [0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...\ndeg_pH10               [2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...\ndeg_Mg_50C             [0.35810000000000003, 2.9683, 0.2589, 1.4552, ...\ndeg_50C                [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...\nName: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train = pd.read_json('data/train.json', lines=True) \n",
    "test = pd.read_json('data/test.json', lines=True) \n",
    "# Divide test data into the two subsets: Private Test and Public Test\n",
    "# seq_length=107 in Public Test while seq_length=130 in Private Test\n",
    "test_public = test[test[\"seq_length\"] == 107]\n",
    "test_private = test[test[\"seq_length\"] == 130]\n",
    "\n",
    "# Print the first sample for testing\n",
    "df = pd.DataFrame(train)\n",
    "print(df.iloc[0])\n",
    "\n",
    "# Optionally, only take training data which have passed the signal-to-noise filter\n",
    "train_filtered = train[train[\"SN_filter\"] == 1]\n",
    "\n",
    "# Change apply_SN_filter to True to only train on filtered data, using the SN filter described in the Kaggle challenge (same which is used for public test data)\n",
    "# As described in the report, we opted to using the provided noise filter rather than using our denoising autoencoder, as this gave better results.\n",
    "apply_SN_filter = True\n",
    "# apply_SN_filter = False \n",
    "if apply_SN_filter == True:\n",
    "    train = train_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struct_adj(data = train, sequential_edges = False):\n",
    "    # Get adjacency matrix from sample structure sequence\n",
    "    # Include edges between base pairs\n",
    "    # If sequential_edges == False, do not include edges between sequential bases\n",
    "    # If sequential_edges == True, add these edges, which correspond to the diagonals -1 and 1 in the adjacency matrix (assuming undirected edges)\n",
    "    struct_adj = []\n",
    "    for ix in range(len(data)):\n",
    "        seq_length = data[\"seq_length\"].iloc[ix]\n",
    "        structure = data[\"structure\"].iloc[ix]\n",
    "        sequence = data[\"sequence\"].iloc[ix]\n",
    "\n",
    "        queue = [] # Store indices corresponding to \"(\" in queue\n",
    "\n",
    "        sample_struct_adj = np.zeros([seq_length, seq_length])\n",
    "        for jx in range(seq_length):\n",
    "            if structure[jx] == \"(\":\n",
    "                queue.append(jx) # Append index of \"(\" in base pair to queue\n",
    "            elif structure[jx] == \")\":\n",
    "                start = queue.pop() # Retrieve index of last \"(\" in queue, corresponding to \")\" at jx\n",
    "                sample_struct_adj[start, jx] = 1 # Add edge from \"(\" to \")\"\n",
    "                sample_struct_adj[jx, start] = 1 # Add edge from \")\" to \"(\" (assume undirected)\n",
    "\n",
    "        if sequential_edges == True:\n",
    "            ones = np.ones(seq_length-1) # Match length of -1 and 1 diagonals in sample_struct_adj\n",
    "            sample_struct_adj += np.diag(ones,1) # Add sequential edges (i,i+1) \n",
    "            sample_struct_adj += np.diag(ones,-1) # Add sequential edges (i+1,i) (assume non-directed)\n",
    "\n",
    "        struct_adj.append(sample_struct_adj)\n",
    "\n",
    "    struct_adj = np.array(struct_adj)\n",
    "    return struct_adj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for constructing distance adjacency matrix\n",
    "# Only returns one distance adjacency matrix, since it is identical for all samples (only depends on number of nodes)\n",
    "def get_dist_adj(data = train, power = 1):\n",
    "    # Get adjacency matrix from inverse index-based distance between nodes\n",
    "    # power is the variable p in the expression D(i,j)\n",
    "    dist_adj = []\n",
    "    idx = np.arange(data[\"seq_length\"].iloc[0]) # Get number of nodes\n",
    "    for ix in range(len(idx)):\n",
    "        d = np.abs(idx[ix] - idx) # Get distance from individual nodes to all other nodes\n",
    "        dist_adj.append(d)\n",
    "\n",
    "    # Convert distance to distance measure according to formula    \n",
    "    dist_adj = np.array(dist_adj) + 1 # Add one to avoid singularity at d=0\n",
    "    dist_adj = 1/dist_adj # Inverse of distance\n",
    "    dist_adj = dist_adj**power # Apply the specified power\n",
    "    return dist_adj "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base pair probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/1589 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f045130cfc7345f0983ae76a057c0277"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/629 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8be3adfebcba4b3099a6d4e606d1a27f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Load the provided base pair probability adjacency matrices for the samples included in the datasets\n",
    "\n",
    "# Train\n",
    "Adj_bpps = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps.append(bpps)\n",
    "Adj_bpps = np.array(Adj_bpps)\n",
    "\n",
    "# Public test\n",
    "Adj_bpps_test_public = []\n",
    "for id in tqdm(test_public[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps_test_public.append(bpps)\n",
    "Adj_bpps_test_public = np.array(Adj_bpps_test_public)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(data = train):\n",
    "    # Create a node feature matrix for each sample in data\n",
    "    # Encode feature vectors as one-hot arrays  \n",
    "    # Included features: \n",
    "    #   Base (given by sequence)\n",
    "    #   Loop type (given by predicted_loop_type)\n",
    "    # Could also include sequence, i.e. \".\" \"(\" and \")\", but I don't see how this provides any interesting information if the structure adjacency matrix is used\n",
    "    X = [] # Stacked node feature matrices for all samples in data\n",
    "    \n",
    "    for ix in range(len(data)):\n",
    "        seq_length = data[\"seq_length\"].iloc[ix]\n",
    "        sequence = data[\"sequence\"].iloc[ix]\n",
    "        predicted_loop_type = data[\"predicted_loop_type\"].iloc[ix]\n",
    "\n",
    "        X_sample = [] # Node feature matrix for current sample\n",
    "\n",
    "        for jx in range(seq_length):\n",
    "            # Base one hot\n",
    "            bases = np.array(['A', 'G', 'U', 'C']) # Different order than reference notebook (A,G,C,U)\n",
    "            x_base = np.zeros(len(bases))\n",
    "            x_base[bases == sequence[jx]] = 1 # Set base one-hot to 1 at correct index\n",
    "\n",
    "            # Predicted Loop Type one hot\n",
    "            loop_types = np.array(['S', 'M', 'I', 'B', 'H', 'E', 'X'])\n",
    "            x_loop = np.zeros(len(loop_types))\n",
    "            x_loop[loop_types == predicted_loop_type[jx]] = 1 # Set loop-type one-hot to 1 at correct index\n",
    "\n",
    "            x = np.concatenate((x_base,x_loop)) # Concatenate to one node feature vector\n",
    "            X_sample.append(x) # Append node feature vector to node feature matrix\n",
    "        X_sample = np.array(X_sample)\n",
    "        X.append(X_sample) # Append node feature matrix for current graph\n",
    "    X = np.array(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and pretrain denoise model and encode targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1350, 340)\n85\n85\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "import torch\n",
    "\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\n",
    "error_labels = [\"reactivity_error\", \"deg_error_Mg_pH10\", \"deg_error_Mg_50C\", \"deg_error_pH10\", \"deg_error_50C\"]\n",
    "\n",
    "train_ae = train[train.signal_to_noise > 4].reset_index(drop = True) # remove noisy data\n",
    "y_train_ae = []\n",
    "\n",
    "# Construct target labels\n",
    "for target in target_labels:\n",
    "    y_ae = np.vstack(train_ae[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_train_ae.append(y_ae) # Append array for each target\n",
    "y_train_ae = np.stack(y_train_ae, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_train_ae = y_train_ae.reshape(y_train_ae.shape[0],-1).astype(float)\n",
    "print(y_train_ae.shape)\n",
    "\n",
    "# Construct error labels\n",
    "y_error_ae = []\n",
    "for label in error_labels:\n",
    "    y_ae = np.vstack(train_ae[label]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_error_ae.append(y_ae) # Append array for each target\n",
    "y_error_ae = np.stack(y_error_ae, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_error_ae = y_error_ae.reshape(y_error_ae.shape[0],-1).astype(float) # flatten and cast to float\n",
    "\n",
    "\n",
    "y_train_loader = DataLoader(y_train_ae, batch_size=16, shuffle=True)\n",
    "y_error_loader = DataLoader(y_error_ae, batch_size=16, shuffle=True)\n",
    "print(len(y_train_loader))\n",
    "print(len(y_error_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class AE(nn.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(AE,self).__init__()\n",
    "    torch.manual_seed(12345) # For reproducible results\n",
    "\n",
    "    self.encoder=nn.Sequential(\n",
    "                  nn.Linear(kwargs[\"input_shape\"],512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,512),\n",
    "                  #nn.ReLU(True)\n",
    "        \n",
    "                  )\n",
    "    \n",
    "    self.decoder=nn.Sequential(\n",
    "                  nn.Linear(512,512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,512),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(512,kwargs[\"input_shape\"]),\n",
    "                  )\n",
    "    \n",
    " \n",
    "  def forward(self,x):\n",
    "    x = F.dropout(x, p=0.4, training=True)\n",
    "    x=self.encoder(x)\n",
    "    x=self.decoder(x)\n",
    "    \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2,        101] loss: 0.02427\n",
      "[3,        201] loss: 0.04576\n",
      "[4,        301] loss: 0.06072\n",
      "[5,        401] loss: 0.07357\n",
      "[6,        501] loss: 0.08695\n",
      "[8,        601] loss: 0.00552\n",
      "[9,        701] loss: 0.02099\n",
      "[10,        801] loss: 0.03492\n",
      "[11,        901] loss: 0.04768\n",
      "[12,       1001] loss: 0.06101\n",
      "[13,       1101] loss: 0.07501\n",
      "[15,       1201] loss: 0.00830\n",
      "[16,       1301] loss: 0.02205\n",
      "[17,       1401] loss: 0.03406\n",
      "[18,       1501] loss: 0.04649\n",
      "[19,       1601] loss: 0.05780\n",
      "[20,       1701] loss: 0.06962\n",
      "[22,       1801] loss: 0.01159\n",
      "[23,       1901] loss: 0.02291\n",
      "[24,       2001] loss: 0.03413\n",
      "[25,       2101] loss: 0.04524\n",
      "[26,       2201] loss: 0.05667\n",
      "[28,       2301] loss: 0.00335\n",
      "[29,       2401] loss: 0.01395\n",
      "[30,       2501] loss: 0.02523\n",
      "[31,       2601] loss: 0.03545\n",
      "[32,       2701] loss: 0.04580\n",
      "[33,       2801] loss: 0.05611\n",
      "[35,       2901] loss: 0.00692\n",
      "[36,       3001] loss: 0.01726\n",
      "[37,       3101] loss: 0.02711\n",
      "[38,       3201] loss: 0.03641\n",
      "[39,       3301] loss: 0.04550\n",
      "[40,       3401] loss: 0.05577\n",
      "[42,       3501] loss: 0.00923\n",
      "[43,       3601] loss: 0.01862\n",
      "[44,       3701] loss: 0.02795\n",
      "[45,       3801] loss: 0.03714\n",
      "[46,       3901] loss: 0.04549\n",
      "[48,       4001] loss: 0.00297\n",
      "[49,       4101] loss: 0.01166\n",
      "[50,       4201] loss: 0.02008\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Train AE_model\n",
    "\n",
    "autoencoder = AE(input_shape=340)\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "EPOCHS = 50\n",
    "i = 0\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data, label_errors in zip(y_train_loader, y_error_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        \n",
    "        \n",
    "        #construct random tensor [-1,1]\n",
    "        rand_array = torch.rand(size=(label_errors.shape[0], label_errors.shape[1]))*4-2 \n",
    "        \n",
    "        label_errors = torch.mul(label_errors.float(), rand_array.float())\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        x = data + label_errors\n",
    "        # forward + backward + optimize\n",
    "        outputs = autoencoder(x.float())\n",
    "        loss = mse(data.float(), outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        i = i + 1\n",
    "        if i % 100 == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %10d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct target arrays for training data\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\n",
    "\n",
    "y_train = []\n",
    "seq_length = train[\"seq_length\"].iloc[0] # Get number of nodes (lenght of sequence)\n",
    "seq_scored = train[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "save_index = []\n",
    "for i in range(len(train)):\n",
    "    if float(train[\"signal_to_noise\"].iloc[i]) <= 1:\n",
    "           save_index.append(i)\n",
    "for target in target_labels:\n",
    "    y = np.vstack(train[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_train.append(y) # Append array for each target\n",
    "y_train = np.stack(y_train, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_train = y_train.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Shape of targets: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2400, 68, 5)\n",
      "(2400, 68, 5)\n"
     ]
    }
   ],
   "source": [
    "# As described in the report, applying the autoencoder to the targets was ignored when generating the final results. \r\n",
    "\r\n",
    "autoencoder.eval()\r\n",
    "print(y_train.shape)\r\n",
    "for i in save_index:\r\n",
    "    y_train_tensor = torch.Tensor(y_train[i,:,:].astype(float).flatten())\r\n",
    "    \r\n",
    "    outputs = autoencoder(y_train_tensor)   \r\n",
    "    outputs = outputs.reshape(68,5)\r\n",
    "    y_train[i,:,:] = outputs.detach().numpy() \r\n",
    "print(y_train.shape)"
   ]
  },
  {
   "source": [
    "## Construct targets without applying denoising autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of targets:  (1589, 68, 5)\n"
     ]
    }
   ],
   "source": [
    "# Construct target arrays for training data\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\", \"deg_pH10\", \"deg_50C\"]\n",
    "\n",
    "y_train = []\n",
    "seq_length = train[\"seq_length\"].iloc[0] # Get number of nodes (lenght of sequence)\n",
    "seq_scored = train[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "for target in target_labels:\n",
    "    y = np.vstack(train[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_train.append(y) # Append array for each target\n",
    "y_train = np.stack(y_train, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_train = y_train.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Shape of targets: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MCRMSE loss function\n",
    "# Include all 5 targets by default, allow optional argument to calculate MCRMSE of scored targets only.\n",
    "# Assumes targets are ordered such that the first 3 targets are the scored ones.\n",
    "# Inputs should have dimensions (n_samples, n_nodes, n_targets)\n",
    "def MCRMSE(y_true, y_pred, only_scored=False, data = train, all_102 = False):\n",
    "    # Reshape if input only includes one sample and has dimensions (n_nodes, n_targets)\n",
    "    if y_true.dim() == 2:\n",
    "        y_true = y_true[None, :, :]\n",
    "    if y_pred.dim() == 2:\n",
    "        y_pred = y_pred[None, :, :]\n",
    "\n",
    "    # Extract the scored targets\n",
    "    seq_scored = data[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "    if all_102 == True:\n",
    "        seq_scored = 102\n",
    "    y_pred = y_pred[:, :seq_scored, :] \n",
    "    # true = y_true[:, :seq_scored, :] # Not necessary since only scored targets are included, could include dummy values instead as in reference notebook\n",
    "\n",
    "    y_diff = y_pred - y_true\n",
    "    mse = torch.mean(y_diff**2, axis=1) # Average over nodes in each sample for every target\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    num_scored = 5 # Include all targets by default\n",
    "    if only_scored == True:\n",
    "        num_scored = 3 # Include only scored targets if specified by keyword (assumes correct ordering of targets in y_true and y_pred)\n",
    "\n",
    "    mcrmse = torch.mean(rmse[:, :num_scored], axis=1) # Average over included targets\n",
    "\n",
    "    return mcrmse"
   ]
  },
  {
   "source": [
    "## Load test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "id                                                           id_40f52a81b\nID                                                               10207086\nsequence                GGAAAUUUUCGCGGGACGGGCGGCCGGGCGGAGGCGGCGCGAGGGC...\nstructure               .......(((((.((.((..(.(((..(((...((..((((....(...\nseqpos                  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\nreactivity              [0.6009, 1.3193, 1.5475, 0.5852, 1.566, 0.3387...\ndeg_Mg_pH10             [0.5866, 1.4956, 1.3765, 0.5714, 2.9199, 0.925...\ndeg_Mg_50C              [1.2183, 1.718, 0.8737, 0.9644, 2.7502, 0.5862...\nerrors                  [0.4419, 0.4736, 0.4529, 0.30820000000000003, ...\ndeg_pH10_Mg_errors      [0.4333, 0.491, 0.4375, 0.3079, 0.5534, 0.3432...\ndeg_50C_Mg_errors       [0.5922000000000001, 0.5873, 0.434700000000000...\nS/N filter                                                              1\npredicted_loop_type     EEEEEEESSSSSISSISSIISISSSIISSSIIISSIISSSSIIIIS...\nseq_scored                                                             92\nseq_length                                                            130\ncluster_id                                                            694\nn_neighbors                                                             2\nfirst_cluster_member                                                    1\ntest_filter                                                             1\nName: 0, dtype: object\nNumber of samples in private_test_labels.csv:  2493\nNumber of samples in private_test_labels.csv with S/N_filter=1:  2493\nNumber of samples in private_test_labels.csv with test_filter=1:  1172\n"
     ]
    }
   ],
   "source": [
    "# Load post deadline data\n",
    "test_postdeadline = pd.read_csv('data/post_deadline_files/private_test_labels.csv') \n",
    "# Print the first sample for testing\n",
    "df_pd = pd.DataFrame(test_postdeadline)\n",
    "print(df_pd.iloc[0])\n",
    "print(\"Number of samples in private_test_labels.csv: \", len(test_postdeadline))\n",
    "# There are two filters, one is S/N filter, which should be 1 for all samples in test_postdeadline (but not for all samples in test_private, \"pre deadline\")\n",
    "# This is because the private test data was changed, they decided to include the filters there as well. \n",
    "# To my understanding, S/N filter is 1 for private test data which \n",
    "test_postdeadline_SNfiltered = test_postdeadline[test_postdeadline[\"S/N filter\"] == 1]\n",
    "print(\"Number of samples in private_test_labels.csv with S/N_filter=1: \", len(test_postdeadline_SNfiltered))\n",
    "# Note that there is a test_filter variable, which is not 1 for all samples\n",
    "# Not sure what this filter indicates, could be if all filters are passed\n",
    "test_postdeadline_filtered = test_postdeadline[test_postdeadline[\"test_filter\"] == 1]\n",
    "print(\"Number of samples in private_test_labels.csv with test_filter=1: \", len(test_postdeadline_filtered))\n",
    "\n",
    "# Set apply_test_filter to true to set private post deadline test data to only be samples which pass the filter\n",
    "# This should probably not be done, as the private test data is \"unseen\" data, where we should have no information about the targets\n",
    "apply_test_filter = False\n",
    "# apply_test_filter = True\n",
    "if apply_test_filter == True:\n",
    "    test_postdeadline = test_postdeadline_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2493 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cf41728547e4eaa87fc1e487561dcac"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Load the provided base pair probability adjacency matrices for the samples in post deadline private test\n",
    "\n",
    "# Private test, post deadline\n",
    "Adj_bpps_test_postdeadline = []\n",
    "for id in tqdm(test_postdeadline[\"id\"]):\n",
    "    bpps = np.load(f\"data/bpps/{id}.npy\")\n",
    "    Adj_bpps_test_postdeadline.append(bpps)\n",
    "Adj_bpps_test_postdeadline = np.array(Adj_bpps_test_postdeadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of targets:  (2493, 92, 3)\n"
     ]
    }
   ],
   "source": [
    "# Construct target arrays for post deadline private test data\n",
    "target_labels = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n",
    "\n",
    "y_test_postdeadline = []\n",
    "seq_length = test_postdeadline[\"seq_length\"].iloc[0] # Get number of nodes (lenght of sequence)\n",
    "seq_scored = test_postdeadline[\"seq_scored\"].iloc[0] # Get number of nodes with ground truth targets\n",
    "# seq_scored = 102\n",
    "for target in target_labels:\n",
    "    y_strings = np.vstack(test_postdeadline[target]) # Create (n_samples, seq_scored) arrays for each target\n",
    "    y_temp = [] # For converting strings to arrays\n",
    "    # Post deadline targets are given as strings, iterate over samples and convert strings to arrays\n",
    "    for ix in range(len(y_strings)): \n",
    "        y_strings[ix][0] = y_strings[ix][0].replace('[','') # Remove first bracket character\n",
    "        y_strings[ix][0] = y_strings[ix][0].replace(']','') # Remove final bracket character\n",
    "        y_temp.append(y_strings[ix][0].split(\",\")) # Split the comma separated string\n",
    "    y_test_postdeadline.append(np.array(y_temp)) # Append array for each target\n",
    "y_test_postdeadline = np.stack(y_test_postdeadline, axis=2) # Join the target arrays along last axis to match shape of feature arrays\n",
    "y_test_postdeadline = y_test_postdeadline.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "# Post deadline data includes more targets (102) than seq_scored indicates (92). Remove the extra values for now, only include up to seq_scored=92\n",
    "y_test_postdeadline = y_test_postdeadline[:,:seq_scored,:]\n",
    "print(\"Shape of targets: \", y_test_postdeadline.shape)\n"
   ]
  },
  {
   "source": [
    "# Train and evaluate models\n",
    "The following cells are used to define the models, train them and evaluate them on the test set. Note that the models are overwritten and that most of the cells are copied with minor adjustments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# GCN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Train\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (1589, 107, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (1589, 107, 107, 2)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for training data\n",
    "print(\"Shapes of inputs - Train\")\n",
    "\n",
    "# Feature exctraction for the GCN models\n",
    "# A includes base pairs and sequential neighbors\n",
    "# B includes bpps\n",
    "# No distance matrices used\n",
    "\n",
    "# Node features\n",
    "X = get_node_features(data = train)\n",
    "X = X.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs = get_struct_adj(data = train, sequential_edges=True)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj = np.concatenate([Adj_pairs[:,:,:,None], Adj_bpps[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension \n",
    "Adj = Adj.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGraphConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Adjuster from the graph neural network operator from the “Weisfeiler and Leman Go \n",
    "    Neural: Higher-order Graph Neural Networks” paper\n",
    "\n",
    "    x' = x_i W_0.T + (Adj_1 x_i) W_1.T + ... +  (Adj_k x_i) W_k.T\n",
    "\n",
    "    Contributions from additional edge features are added as separate terms\n",
    "\n",
    "    Arguments:\n",
    "        in_channels (int): Number of features (size) of each input node\n",
    "        out_channels (int): Number of features (size) of each output node\n",
    "        n_edge_features (int): Number of edge features, i.e. Adj.shape[-1]\n",
    "    \n",
    "    forward performs the graph neural network operation\n",
    "    Arguments:\n",
    "        x (torch tensor): The input node features of shape (n_samples, n_nodes, in_channels) \n",
    "        Adj (torch tensor): The adjacency matrix of the graph of shape (n_samples, n_nodes, n_nodes, n_edge_features)\n",
    "    Returns: \n",
    "        x' (torch tensor): Output node feature matrix of shape (n_samples, n_nodes, out_channels)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, n_edge_features):\n",
    "        super(myGraphConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.n_edge_features = n_edge_features # Get number of edge features (number of stacked adjacency matrices)\n",
    "\n",
    "        self.lin_self = Linear(in_channels, out_channels, bias=True) \n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1 = Linear(in_channels, out_channels, bias=True)  \n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 4:\n",
    "            self.lin_4 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 5:\n",
    "            self.lin_5 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 6:\n",
    "            raise ValueError(\"Number of edge features can not be larger than 5\") # \"Hard code\" up to 5 edge features\n",
    "\n",
    "        self.reset_parameters()\n",
    "   \n",
    "    def reset_parameters(self):\n",
    "        self.lin_self.reset_parameters()\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1.reset_parameters()\n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2.reset_parameters()\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3.reset_parameters()\n",
    "        if self.n_edge_features >= 4:\n",
    "            self.lin_4.reset_parameters()\n",
    "        if self.n_edge_features >= 5:\n",
    "            self.lin_5.reset_parameters()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # Shapes of arguments, weight matrices and output\n",
    "        # x: (n_samples, n_nodes, in_channels) \n",
    "        # Adj: (n_samples, n_nodes, n_nodes, n_edge_features)\n",
    "        # W_1: (in_channels, out_channels)\n",
    "        # W_2: (in_channels, out_channels)\n",
    "        # out: (n_samples, n_nodes, out_channels)\n",
    "\n",
    "        # Confirm that the input variable n_edge_features matches the adjacency matrix\n",
    "        if self.n_edge_features != Adj.shape[-1]:\n",
    "            raise ValueError(\"Specified number of edge features must match last dimensino in adjacency matrix\") \n",
    "\n",
    "        # Calculate contribution from self (node)\n",
    "        out = self.lin_self(x)\n",
    "\n",
    "        # Add contributions from edges\n",
    "        # Calculate contributions from adjacent nodes\n",
    "        # Use separate weights for each edge feature\n",
    "        if self.n_edge_features >= 1:\n",
    "            out_1 = torch.matmul(Adj[..., 0], x) # This is equivalent to summing over edge weights assuming Adj contains the edge weights\n",
    "            out_1 = self.lin_1(out_1) # Multiply with weight matrix for adjacent nodes\n",
    "            out += out_1 # Add contribution from first edge feature\n",
    "        # Repeat for all edge weights\n",
    "        if self.n_edge_features >= 2:\n",
    "            out_2 = torch.matmul(Adj[..., 1], x) \n",
    "            out_2 = self.lin_2(out_2) \n",
    "            out += out_2 # Add contribution from second edge feature\n",
    "        if self.n_edge_features >= 3:\n",
    "            out_3 = torch.matmul(Adj[..., 2], x)\n",
    "            out_3 = self.lin_3(out_3) \n",
    "            out += out_3 # Add contribution from third edge feature\n",
    "        if self.n_edge_features >= 4:\n",
    "            out_4 = torch.matmul(Adj[..., 3], x)\n",
    "            out_4 = self.lin_4(out_4) \n",
    "            out += out_4 # Add contribution from fourth edge feature\n",
    "        if self.n_edge_features >= 5:\n",
    "            out_5 = torch.matmul(Adj[..., 4], x)\n",
    "            out_5 = self.lin_5(out_5) \n",
    "            out += out_5 # Add contribution from fifth edge feature\n",
    "        return out\n",
    "\n",
    "    # The method that returns a printable representation of the operator, copy to match GraphConv source code \n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GCN models, where the number of hidden layers can be varied\n",
    "# Note that code has to be commented to change the number of layers (sorry)\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, n_edge_features, n_node_features = 11):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345) # For reproducible results\n",
    "        self.conv = myGraphConv(n_node_features, hidden_channels, n_edge_features)\n",
    "        hidden_channels2 = 64\n",
    "        self.conv2 = myGraphConv(hidden_channels, hidden_channels2, n_edge_features)\n",
    "        hidden_channels3 = 64\n",
    "        self.conv3 = myGraphConv(hidden_channels2, hidden_channels3, n_edge_features)\n",
    "        # hidden_channels4 = 64\n",
    "        # self.conv4 = myGraphConv(hidden_channels3, hidden_channels4, n_edge_features)\n",
    "        # hidden_channels5 = 64\n",
    "        # self.conv5 = myGraphConv(hidden_channels4, hidden_channels5, n_edge_features)\n",
    "        hidden_channels = hidden_channels3\n",
    "        self.lin = Linear(hidden_channels, 5) # Map to the 5 output targets with dense layer\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # 1. Obtain node embeddings, use GraphConv layers with ReLU for non-linearity\n",
    "        x = self.conv(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # x = self.conv4(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        # x = self.conv5(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # No pooling is required, we want target labels for each node, not for the entire graph\n",
    "\n",
    "        # 3. Apply a final classifier \n",
    "        # Use a single layer as classifier to map to the targets\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # No LogSoftmax needed, possibly some other function to map to correct targets?\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNN(\n",
      "  (conv): myGraphConv(11, 64)\n",
      "  (conv2): myGraphConv(64, 64)\n",
      "  (conv3): myGraphConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "=== Starting epoch 1 ===\n",
      "[batch     9, sample   144] loss: 0.597\n",
      "[batch    18, sample   288] loss: 0.442\n",
      "[batch    27, sample   432] loss: 0.403\n",
      "[batch    36, sample   576] loss: 0.375\n",
      "[batch    45, sample   720] loss: 0.377\n",
      "[batch    54, sample   864] loss: 0.363\n",
      "[batch    63, sample  1008] loss: 0.364\n",
      "[batch    72, sample  1152] loss: 0.355\n",
      "[batch    81, sample  1296] loss: 0.371\n",
      "[batch    90, sample  1440] loss: 0.358\n",
      "[batch    99, sample  1584] loss: 0.346\n",
      "=== Starting epoch 2 ===\n",
      "[batch     9, sample   144] loss: 0.378\n",
      "[batch    18, sample   288] loss: 0.353\n",
      "[batch    27, sample   432] loss: 0.335\n",
      "[batch    36, sample   576] loss: 0.332\n",
      "[batch    45, sample   720] loss: 0.338\n",
      "[batch    54, sample   864] loss: 0.331\n",
      "[batch    63, sample  1008] loss: 0.328\n",
      "[batch    72, sample  1152] loss: 0.334\n",
      "[batch    81, sample  1296] loss: 0.344\n",
      "[batch    90, sample  1440] loss: 0.346\n",
      "[batch    99, sample  1584] loss: 0.331\n",
      "=== Starting epoch 3 ===\n",
      "[batch     9, sample   144] loss: 0.368\n",
      "[batch    18, sample   288] loss: 0.342\n",
      "[batch    27, sample   432] loss: 0.325\n",
      "[batch    36, sample   576] loss: 0.323\n",
      "[batch    45, sample   720] loss: 0.328\n",
      "[batch    54, sample   864] loss: 0.325\n",
      "[batch    63, sample  1008] loss: 0.321\n",
      "[batch    72, sample  1152] loss: 0.329\n",
      "[batch    81, sample  1296] loss: 0.335\n",
      "[batch    90, sample  1440] loss: 0.338\n",
      "[batch    99, sample  1584] loss: 0.324\n",
      "=== Starting epoch 4 ===\n",
      "[batch     9, sample   144] loss: 0.362\n",
      "[batch    18, sample   288] loss: 0.336\n",
      "[batch    27, sample   432] loss: 0.321\n",
      "[batch    36, sample   576] loss: 0.319\n",
      "[batch    45, sample   720] loss: 0.324\n",
      "[batch    54, sample   864] loss: 0.321\n",
      "[batch    63, sample  1008] loss: 0.316\n",
      "[batch    72, sample  1152] loss: 0.325\n",
      "[batch    81, sample  1296] loss: 0.330\n",
      "[batch    90, sample  1440] loss: 0.334\n",
      "[batch    99, sample  1584] loss: 0.320\n",
      "=== Starting epoch 5 ===\n",
      "[batch     9, sample   144] loss: 0.358\n",
      "[batch    18, sample   288] loss: 0.332\n",
      "[batch    27, sample   432] loss: 0.317\n",
      "[batch    36, sample   576] loss: 0.316\n",
      "[batch    45, sample   720] loss: 0.320\n",
      "[batch    54, sample   864] loss: 0.317\n",
      "[batch    63, sample  1008] loss: 0.313\n",
      "[batch    72, sample  1152] loss: 0.322\n",
      "[batch    81, sample  1296] loss: 0.325\n",
      "[batch    90, sample  1440] loss: 0.331\n",
      "[batch    99, sample  1584] loss: 0.317\n",
      "=== Starting epoch 6 ===\n",
      "[batch     9, sample   144] loss: 0.355\n",
      "[batch    18, sample   288] loss: 0.329\n",
      "[batch    27, sample   432] loss: 0.314\n",
      "[batch    36, sample   576] loss: 0.312\n",
      "[batch    45, sample   720] loss: 0.318\n",
      "[batch    54, sample   864] loss: 0.314\n",
      "[batch    63, sample  1008] loss: 0.309\n",
      "[batch    72, sample  1152] loss: 0.321\n",
      "[batch    81, sample  1296] loss: 0.324\n",
      "[batch    90, sample  1440] loss: 0.328\n",
      "[batch    99, sample  1584] loss: 0.314\n",
      "=== Starting epoch 7 ===\n",
      "[batch     9, sample   144] loss: 0.352\n",
      "[batch    18, sample   288] loss: 0.328\n",
      "[batch    27, sample   432] loss: 0.313\n",
      "[batch    36, sample   576] loss: 0.310\n",
      "[batch    45, sample   720] loss: 0.315\n",
      "[batch    54, sample   864] loss: 0.313\n",
      "[batch    63, sample  1008] loss: 0.306\n",
      "[batch    72, sample  1152] loss: 0.319\n",
      "[batch    81, sample  1296] loss: 0.321\n",
      "[batch    90, sample  1440] loss: 0.326\n",
      "[batch    99, sample  1584] loss: 0.312\n",
      "=== Starting epoch 8 ===\n",
      "[batch     9, sample   144] loss: 0.350\n",
      "[batch    18, sample   288] loss: 0.326\n",
      "[batch    27, sample   432] loss: 0.310\n",
      "[batch    36, sample   576] loss: 0.308\n",
      "[batch    45, sample   720] loss: 0.314\n",
      "[batch    54, sample   864] loss: 0.312\n",
      "[batch    63, sample  1008] loss: 0.304\n",
      "[batch    72, sample  1152] loss: 0.316\n",
      "[batch    81, sample  1296] loss: 0.318\n",
      "[batch    90, sample  1440] loss: 0.323\n",
      "[batch    99, sample  1584] loss: 0.310\n",
      "=== Starting epoch 9 ===\n",
      "[batch     9, sample   144] loss: 0.348\n",
      "[batch    18, sample   288] loss: 0.325\n",
      "[batch    27, sample   432] loss: 0.308\n",
      "[batch    36, sample   576] loss: 0.307\n",
      "[batch    45, sample   720] loss: 0.313\n",
      "[batch    54, sample   864] loss: 0.310\n",
      "[batch    63, sample  1008] loss: 0.302\n",
      "[batch    72, sample  1152] loss: 0.315\n",
      "[batch    81, sample  1296] loss: 0.316\n",
      "[batch    90, sample  1440] loss: 0.322\n",
      "[batch    99, sample  1584] loss: 0.309\n",
      "=== Starting epoch 10 ===\n",
      "[batch     9, sample   144] loss: 0.346\n",
      "[batch    18, sample   288] loss: 0.323\n",
      "[batch    27, sample   432] loss: 0.307\n",
      "[batch    36, sample   576] loss: 0.305\n",
      "[batch    45, sample   720] loss: 0.312\n",
      "[batch    54, sample   864] loss: 0.308\n",
      "[batch    63, sample  1008] loss: 0.300\n",
      "[batch    72, sample  1152] loss: 0.313\n",
      "[batch    81, sample  1296] loss: 0.315\n",
      "[batch    90, sample  1440] loss: 0.321\n",
      "[batch    99, sample  1584] loss: 0.308\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GCN model, optimizer and loss function\n",
    "model = GNN(hidden_channels=64, n_edge_features = Adj.shape[-1])\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Adjust learning rate\n",
    "criterion = MCRMSE # Mean column-wise root mean square error (MCRMSE) loss\n",
    "\n",
    "# Define trainer function for GNN\n",
    "def run_training(X_data, Adj_data, batch_size = 1, n_epochs = 1):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"=== Starting epoch {epoch + 1} ===\")\n",
    "        # Get permutation of sample indices for shuffling\n",
    "        permutation = torch.randperm(len(X_data))\n",
    "        # permutation = range(len(X_data))\n",
    "        # Define variable for printing training loss\n",
    "        running_loss = 0.\n",
    "        # Run training over mini-batches for current epoch\n",
    "        for ix in range(0, len(X_data), batch_size):  # Iterate over samples in the training dataset\n",
    "            batch_indices = permutation[ix:ix+batch_size] # Get shuffled indices for minibatch\n",
    "            X_batch, Adj_batch = X_data[batch_indices], Adj_data[batch_indices] # Minibatch of X and Adj\n",
    "            y_batch = y_train[batch_indices] # Minibatch of ground truths\n",
    "            out = model(X_batch, Adj_batch) # Perform forward pass\n",
    "            loss = criterion(torch.tensor(y_batch), out)  # Compute the loss\n",
    "            # As the loss function is defined per sample, we have to reduce the loss for\n",
    "            # each mini-batch to a singular value in some way.\n",
    "            # Could for example use mean, sum, or random sample. This is a design choice.\n",
    "            loss = torch.mean(loss) # Calculate average loss for minibatch\n",
    "            loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            # Print statistics every print_batch minibatches \n",
    "            print_batch = int(len(X_data)/10/batch_size) # Set to print every 1/10 of all samples\n",
    "            running_loss += loss.item() # Add (average) loss from minibatch\n",
    "            if int(ix/batch_size) % print_batch == 0 and ix != 0: # Ignore first minibatch\n",
    "                print('[batch %5d, sample %5d] loss: %.3f' % \n",
    "                        (int(ix/batch_size), ix, \n",
    "                        running_loss / print_batch)) # Average running loss\n",
    "                running_loss = 0. # Reset running loss\n",
    "\n",
    "# Convert training data inputs to pytorch tensors and run training\n",
    "X_torch = torch.tensor(X)\n",
    "Adj_torch = torch.tensor(Adj)\n",
    "run_training(X_torch, Adj_torch, batch_size = 16, n_epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def run_prediction(X_data, Adj_data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    outs = model(X_data, Adj_data) # Feed the data through the network \n",
    "    for yx in outs:\n",
    "        y_pred.append(yx.detach().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Private test, post deadline\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (2493, 130, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130, 3)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (2493, 130, 130, 2)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for post deadline test data\n",
    "print(\"Shapes of inputs - Private test, post deadline\")\n",
    "\n",
    "# GCN\n",
    "# A includes base pairs and sequential neighbors\n",
    "# B includes bpps\n",
    "# No distance matrices used\n",
    "\n",
    "# Node features\n",
    "X_test_postdeadline = get_node_features(data = test_postdeadline)\n",
    "X_test_postdeadline = X_test_postdeadline.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X_test_postdeadline.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs_test_postdeadline = get_struct_adj(data = test_postdeadline, sequential_edges=True)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs_test_postdeadline.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps_test_postdeadline.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj_test_postdeadline = np.concatenate([Adj_pairs_test_postdeadline[:,:,:,None], Adj_bpps_test_postdeadline[:,:,:,None]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj_test_postdeadline = Adj_test_postdeadline.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj_test_postdeadline.shape)"
   ]
  },
  {
   "source": [
    "## Results for 1-layer GCN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.33861\nMean score on training data, only scored targets: 0.34802\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")\n",
    "\n",
    "# Training score\n",
    "# 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on private test data, only scored targets: 0.43396\nNumber of trainable model parameters:  2629\n"
     ]
    }
   ],
   "source": [
    "X_test_postdeadline_torch = torch.tensor(X_test_postdeadline)\n",
    "Adj_test_postdeadline_torch = torch.tensor(Adj_test_postdeadline)\n",
    "y_postdeadline_pred = run_prediction(X_test_postdeadline_torch, Adj_test_postdeadline_torch)\n",
    "y_postdeadline_pred = y_postdeadline_pred.astype(np.float32)\n",
    "# Post deadline data only includes the scored targets. Remove the two unscored targets from predictions when calculating score\n",
    "y_postdeadline_pred =  y_postdeadline_pred[:,:,:3]# Remove deg_pH10 and deg_50C (unscored targets, to match post deadline test data)\n",
    "\n",
    "# Calculate score on post deadline test data\n",
    "y_test_postdeadline_torch = torch.tensor(y_test_postdeadline)\n",
    "y_postdeadline_pred_torch = torch.tensor(y_postdeadline_pred)\n",
    "postdeadline_score_only_scored = MCRMSE(y_test_postdeadline_torch, y_postdeadline_pred_torch, only_scored=True, data=test_postdeadline)\n",
    "\n",
    "print(f\"Mean score on private test data, only scored targets: {float(torch.mean(postdeadline_score_only_scored)):.5}\")\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print(\"Number of trainable model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# Test score\n",
    "# 5 layers"
   ]
  },
  {
   "source": [
    "## Results for 3-layer GCN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.30659\nMean score on training data, only scored targets: 0.31539\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")\n",
    "\n",
    "# Training score\n",
    "# 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on private test data, only scored targets: 0.41284\nNumber of trainable model parameters:  15109\n"
     ]
    }
   ],
   "source": [
    "X_test_postdeadline_torch = torch.tensor(X_test_postdeadline)\n",
    "Adj_test_postdeadline_torch = torch.tensor(Adj_test_postdeadline)\n",
    "y_postdeadline_pred = run_prediction(X_test_postdeadline_torch, Adj_test_postdeadline_torch)\n",
    "y_postdeadline_pred = y_postdeadline_pred.astype(np.float32)\n",
    "# Post deadline data only includes the scored targets. Remove the two unscored targets from predictions when calculating score\n",
    "y_postdeadline_pred =  y_postdeadline_pred[:,:,:3]# Remove deg_pH10 and deg_50C (unscored targets, to match post deadline test data)\n",
    "\n",
    "# Calculate score on post deadline test data\n",
    "y_test_postdeadline_torch = torch.tensor(y_test_postdeadline)\n",
    "y_postdeadline_pred_torch = torch.tensor(y_postdeadline_pred)\n",
    "postdeadline_score_only_scored = MCRMSE(y_test_postdeadline_torch, y_postdeadline_pred_torch, only_scored=True, data=test_postdeadline)\n",
    "\n",
    "print(f\"Mean score on private test data, only scored targets: {float(torch.mean(postdeadline_score_only_scored)):.5}\")\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print(\"Number of trainable model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# Test score\n",
    "# 3 layers"
   ]
  },
  {
   "source": [
    "## Results for 5-layer GCN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.30503\nMean score on training data, only scored targets: 0.31377\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")\n",
    "\n",
    "# Training score\n",
    "# 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on private test data, only scored targets: 0.4087\nNumber of trainable model parameters:  15109\n"
     ]
    }
   ],
   "source": [
    "X_test_postdeadline_torch = torch.tensor(X_test_postdeadline)\n",
    "Adj_test_postdeadline_torch = torch.tensor(Adj_test_postdeadline)\n",
    "y_postdeadline_pred = run_prediction(X_test_postdeadline_torch, Adj_test_postdeadline_torch)\n",
    "y_postdeadline_pred = y_postdeadline_pred.astype(np.float32)\n",
    "# Post deadline data only includes the scored targets. Remove the two unscored targets from predictions when calculating score\n",
    "y_postdeadline_pred =  y_postdeadline_pred[:,:,:3]# Remove deg_pH10 and deg_50C (unscored targets, to match post deadline test data)\n",
    "\n",
    "# Calculate score on post deadline test data\n",
    "y_test_postdeadline_torch = torch.tensor(y_test_postdeadline)\n",
    "y_postdeadline_pred_torch = torch.tensor(y_postdeadline_pred)\n",
    "postdeadline_score_only_scored = MCRMSE(y_test_postdeadline_torch, y_postdeadline_pred_torch, only_scored=True, data=test_postdeadline)\n",
    "\n",
    "print(f\"Mean score on private test data, only scored targets: {float(torch.mean(postdeadline_score_only_scored)):.5}\")\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print(\"Number of trainable model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# Test score\n",
    "# 5 layers"
   ]
  },
  {
   "source": [
    "# MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct node features and adjacency matrix for training data\n",
    "print(\"Shapes of inputs - Train\")\n",
    "\n",
    "# Feature exctraction for the MLP model\n",
    "\n",
    "# Node features\n",
    "X = get_node_features(data = train)\n",
    "X = X.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X.shape)\n",
    "Adj =  get_struct_adj(data = train, sequential_edges=True) # Not used for the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simple MLP model using self-contribution and no neighbourhood aggregation\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, n_node_features = 11, n_edge_features = None):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(12345) # For reproducible results\n",
    "        self.hidden = Linear(n_node_features, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 5) # Map to the 5 output targets with dense layer\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x, Adj=None):\n",
    "        # 1. Obtain node embeddings, use GraphConv layers with ReLU for non-linearity\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # No pooling is required, we want target labels for each node, not for the entire graph\n",
    "\n",
    "        # 3. Apply a final classifier \n",
    "        # Use a single layer as classifier to map to the targets\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # No LogSoftmax needed, possibly some other function to map to correct targets?\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLP model, optimizer and loss function\n",
    "model = MLP(hidden_channels=64, n_edge_features = 0)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Adjust learning rate\n",
    "criterion = MCRMSE # Mean column-wise root mean square error (MCRMSE) loss\n",
    "\n",
    "# Define trainer function for GNN\n",
    "def run_training(X_data, Adj_data, batch_size = 1, n_epochs = 1):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"=== Starting epoch {epoch + 1} ===\")\n",
    "        # Get permutation of sample indices for shuffling\n",
    "        permutation = torch.randperm(len(X_data))\n",
    "        # permutation = range(len(X_data))\n",
    "        # Define variable for printing training loss\n",
    "        running_loss = 0.\n",
    "        # Run training over mini-batches for current epoch\n",
    "        for ix in range(0, len(X_data), batch_size):  # Iterate over samples in the training dataset\n",
    "            batch_indices = permutation[ix:ix+batch_size] # Get shuffled indices for minibatch\n",
    "            X_batch, Adj_batch = X_data[batch_indices], Adj_data[batch_indices] # Minibatch of X and Adj\n",
    "            y_batch = y_train[batch_indices] # Minibatch of ground truths\n",
    "            out = model(X_batch, Adj_batch) # Perform forward pass\n",
    "            loss = criterion(torch.tensor(y_batch), out)  # Compute the loss\n",
    "            # As the loss function is defined per sample, we have to reduce the loss for\n",
    "            # each mini-batch to a singular value in some way.\n",
    "            # Could for example use mean, sum, or random sample. This is a design choice.\n",
    "            loss = torch.mean(loss) # Calculate average loss for minibatch\n",
    "            loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            # Print statistics every print_batch minibatches \n",
    "            print_batch = int(len(X_data)/10/batch_size) # Set to print every 1/10 of all samples\n",
    "            running_loss += loss.item() # Add (average) loss from minibatch\n",
    "            if int(ix/batch_size) % print_batch == 0 and ix != 0: # Ignore first minibatch\n",
    "                print('[batch %5d, sample %5d] loss: %.3f' % \n",
    "                        (int(ix/batch_size), ix, \n",
    "                        running_loss / print_batch)) # Average running loss\n",
    "                running_loss = 0. # Reset running loss\n",
    "\n",
    "# Convert training data inputs to pytorch tensors and run training\n",
    "X_torch = torch.tensor(X)\n",
    "Adj_torch = torch.tensor(Adj)\n",
    "run_training(X_torch, Adj_torch, batch_size = 16, n_epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def run_prediction(X_data, Adj_data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    outs = model(X_data, Adj_data) # Feed the data through the network \n",
    "    for yx in outs:\n",
    "        y_pred.append(yx.detach().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Private test, post deadline\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (2493, 130, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (2493, 130, 130)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (2493, 130, 130, 2)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for post deadline test data\n",
    "print(\"Shapes of inputs - Private test, post deadline\")\n",
    "\n",
    "# Node features\n",
    "X_test_postdeadline = get_node_features(data = test_postdeadline)\n",
    "X_test_postdeadline = X_test_postdeadline.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X_test_postdeadline.shape)\n",
    "Adj_test_postdeadline =  get_struct_adj(data = test_postdeadline, sequential_edges=True) # Not used for the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.33861\nMean score on training data, only scored targets: 0.34802\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")\n",
    "\n",
    "# Training score\n",
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on private test data, only scored targets: 0.47487\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on test data\n",
    "X_test_postdeadline_torch = torch.tensor(X_test_postdeadline)\n",
    "Adj_test_postdeadline_torch = torch.tensor(Adj_test_postdeadline)\n",
    "y_postdeadline_pred = run_prediction(X_test_postdeadline_torch, Adj_test_postdeadline_torch)\n",
    "y_postdeadline_pred = y_postdeadline_pred.astype(np.float32)\n",
    "# Post deadline data only includes the scored targets. Remove the two unscored targets from predictions when calculating score\n",
    "y_postdeadline_pred =  y_postdeadline_pred[:,:,:3]# Remove deg_pH10 and deg_50C (unscored targets, to match post deadline test data)\n",
    "\n",
    "# Calculate score on post deadline test data\n",
    "y_test_postdeadline_torch = torch.tensor(y_test_postdeadline)\n",
    "y_postdeadline_pred_torch = torch.tensor(y_postdeadline_pred)\n",
    "postdeadline_score_only_scored = MCRMSE(y_test_postdeadline_torch, y_postdeadline_pred_torch, only_scored=True, data=test_postdeadline)\n",
    "\n",
    "print(f\"Mean score on private test data, only scored targets: {float(torch.mean(postdeadline_score_only_scored)):.5}\")\n",
    "# Test score\n",
    "# MLP"
   ]
  },
  {
   "source": [
    "# SIGN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Train\n",
      "Node features X: (n_samples, n_nodes, n_node_features)  (1589, 107, 11)\n",
      "Structure adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Distance adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107, 2)\n",
      "Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes)  (1589, 107, 107)\n",
      "Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features)  (1589, 107, 107, 4)\n"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for training data\n",
    "print(\"Shapes of inputs - Train\")\n",
    "\n",
    "# Feature extraction for the SIGN model\n",
    "# A includes base pairs \n",
    "# B includes bpps\n",
    "# D_i includes distance measure, with power i (use several i)\n",
    "\n",
    "# Node features\n",
    "X = get_node_features(data = train)\n",
    "X = X.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs = get_struct_adj(data = train, sequential_edges=False)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs.shape)\n",
    "# Distance adjacency\n",
    "Adj_dist_1 = get_dist_adj(data = train, power = 1)\n",
    "Adj_dist_2 = get_dist_adj(data = train, power = 2)\n",
    "Adj_dist_3 = get_dist_adj(data = train, power = 3)\n",
    "Adj_dist = np.concatenate([Adj_dist_1[:,:,None], Adj_dist_2[:,:,None], Adj_dist_3[:,:,None]], axis = 2)\n",
    "Adj_dist = Adj_dist[None, :,:,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist = np.repeat(Adj_dist, len(train), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj = np.concatenate([Adj_pairs[:,:,:,None], Adj_bpps[:,:,:,None], Adj_dist[:,:,:,:]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj = Adj.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGraphConv_expand(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative Graph Convolution operation, used in the SIGN architecture.\n",
    "    Here, all neighbourhood aggregators are applied in one layer and are then concatenated to a single vector.\n",
    "\n",
    "    x'_i = [(Adj_1 x_i) W_1.T; (Adj_2 x_i) W_2.T;...]\n",
    "\n",
    "    where [a;b;c] are the arrays a, b and c stacked contiguously\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, n_edge_features):\n",
    "        super(myGraphConv_expand, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.n_edge_features = n_edge_features # Get number of edge features (number of stacked adjacency matrices)\n",
    "\n",
    "        self.lin_self = Linear(in_channels, out_channels, bias=True) # bias=False to match GraphConv? Check source code\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1 = Linear(in_channels, out_channels, bias=True)  \n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 4:\n",
    "            self.lin_4 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 5:\n",
    "            self.lin_5 = Linear(in_channels, out_channels, bias=True)\n",
    "        if self.n_edge_features >= 6:\n",
    "            raise ValueError(\"Number of edge features can not be larger than 5\") # \"Hard code\" up to 5 edge features\n",
    "\n",
    "        self.reset_parameters()\n",
    "   \n",
    "    def reset_parameters(self):\n",
    "        self.lin_self.reset_parameters()\n",
    "        if self.n_edge_features >= 1:\n",
    "            self.lin_1.reset_parameters()\n",
    "        if self.n_edge_features >= 2:\n",
    "            self.lin_2.reset_parameters()\n",
    "        if self.n_edge_features >= 3:\n",
    "            self.lin_3.reset_parameters()\n",
    "        if self.n_edge_features >= 4:\n",
    "            self.lin_4.reset_parameters()\n",
    "        if self.n_edge_features >= 5:\n",
    "            self.lin_5.reset_parameters()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # Confirm that the input variable n_edge_features matches the adjacency matrix\n",
    "        if self.n_edge_features != Adj.shape[-1]:\n",
    "            raise ValueError(\"Specified number of edge features must match last dimensino in adjacency matrix\") \n",
    "\n",
    "        # Calculate contribution from self (node)\n",
    "        out = self.lin_self(x)\n",
    "\n",
    "        # Add contributions from edges\n",
    "        # Calculate contributions from adjacent nodes\n",
    "        # Use separate weights for each edge feature\n",
    "        if self.n_edge_features >= 1:\n",
    "            out_1 = torch.matmul(Adj[..., 0], x) # This is equivalent to summing over edge weights assuming Adj contains the edge weights\n",
    "            out_1 = self.lin_1(out_1) # Multiply with weight matrix for adjacent nodes\n",
    "            out = torch.cat([out, out_1], axis=-1) # Add contribution from first edge feature\n",
    "        # Repeat for all edge weights\n",
    "        if self.n_edge_features >= 2:\n",
    "            out_2 = torch.matmul(Adj[..., 1], x) \n",
    "            out_2 = self.lin_2(out_2) \n",
    "            out = torch.cat([out, out_2], axis=-1) # Add contribution from second edge feature\n",
    "        if self.n_edge_features >= 3:\n",
    "            out_3 = torch.matmul(Adj[..., 2], x)\n",
    "            out_3 = self.lin_3(out_3) \n",
    "            out = torch.cat([out, out_3], axis=-1) # Add contribution from third edge feature\n",
    "        if self.n_edge_features >= 4:\n",
    "            out_4 = torch.matmul(Adj[..., 3], x) \n",
    "            out_4 = self.lin_4(out_4) \n",
    "            out = torch.cat([out, out_4], axis=-1) # Add contribution from fourth edge feature\n",
    "        if self.n_edge_features >= 5:\n",
    "            out_5 = torch.matmul(Adj[..., 4], x)\n",
    "            out_5 = self.lin_5(out_5) \n",
    "            out = torch.cat([out, out_5], axis=-1) # Add contribution from fifth edge feature\n",
    "        return out\n",
    "\n",
    "    # The method that returns a printable representation of the operator, copy to match GraphConv source code \n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels*(1+self.n_edge_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SIGN model\n",
    "class GNN_expand(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, n_edge_features, n_node_features = 11):\n",
    "        super(GNN_expand, self).__init__()\n",
    "        torch.manual_seed(12345) # For reproducible results\n",
    "        self.conv = myGraphConv_expand(n_node_features, hidden_channels, n_edge_features)\n",
    "        hidden_channels2 = 64\n",
    "        self.lin2 = Linear(hidden_channels*(n_edge_features+1), hidden_channels2)#, n_edge_features\n",
    "        self.lin = Linear(hidden_channels2, 5) # Map to the 5 output targets with dense layer\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x, Adj):\n",
    "        # 1. Obtain node embeddings, use GraphConv layers with ReLU for non-linearity\n",
    "        x = self.conv(x, Adj) # Give adjacency matrix instead of edge_index and edge_weight\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        # No pooling is required, we want target labels for each node, not for the entire graph\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu(x)\n",
    "        # 3. Apply a final classifier \n",
    "        # Use a single layer as classifier to map to the targets\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # No LogSoftmax needed, possibly some other function to map to correct targets?\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GNN_expand(\n",
      "  (conv): myGraphConv_expand(11, 320)\n",
      "  (lin2): Linear(in_features=320, out_features=64, bias=True)\n",
      "  (lin): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "=== Starting epoch 1 ===\n",
      "[batch     9, sample   144] loss: 0.541\n",
      "[batch    18, sample   288] loss: 0.446\n",
      "[batch    27, sample   432] loss: 0.397\n",
      "[batch    36, sample   576] loss: 0.372\n",
      "[batch    45, sample   720] loss: 0.378\n",
      "[batch    54, sample   864] loss: 0.366\n",
      "[batch    63, sample  1008] loss: 0.361\n",
      "[batch    72, sample  1152] loss: 0.371\n",
      "[batch    81, sample  1296] loss: 0.366\n",
      "[batch    90, sample  1440] loss: 0.370\n",
      "[batch    99, sample  1584] loss: 0.362\n",
      "=== Starting epoch 2 ===\n",
      "[batch     9, sample   144] loss: 0.404\n",
      "[batch    18, sample   288] loss: 0.377\n",
      "[batch    27, sample   432] loss: 0.351\n",
      "[batch    36, sample   576] loss: 0.347\n",
      "[batch    45, sample   720] loss: 0.354\n",
      "[batch    54, sample   864] loss: 0.344\n",
      "[batch    63, sample  1008] loss: 0.338\n",
      "[batch    72, sample  1152] loss: 0.353\n",
      "[batch    81, sample  1296] loss: 0.343\n",
      "[batch    90, sample  1440] loss: 0.346\n",
      "[batch    99, sample  1584] loss: 0.335\n",
      "=== Starting epoch 3 ===\n",
      "[batch     9, sample   144] loss: 0.374\n",
      "[batch    18, sample   288] loss: 0.355\n",
      "[batch    27, sample   432] loss: 0.331\n",
      "[batch    36, sample   576] loss: 0.324\n",
      "[batch    45, sample   720] loss: 0.331\n",
      "[batch    54, sample   864] loss: 0.322\n",
      "[batch    63, sample  1008] loss: 0.318\n",
      "[batch    72, sample  1152] loss: 0.335\n",
      "[batch    81, sample  1296] loss: 0.325\n",
      "[batch    90, sample  1440] loss: 0.331\n",
      "[batch    99, sample  1584] loss: 0.321\n",
      "=== Starting epoch 4 ===\n",
      "[batch     9, sample   144] loss: 0.361\n",
      "[batch    18, sample   288] loss: 0.343\n",
      "[batch    27, sample   432] loss: 0.327\n",
      "[batch    36, sample   576] loss: 0.317\n",
      "[batch    45, sample   720] loss: 0.321\n",
      "[batch    54, sample   864] loss: 0.315\n",
      "[batch    63, sample  1008] loss: 0.311\n",
      "[batch    72, sample  1152] loss: 0.327\n",
      "[batch    81, sample  1296] loss: 0.317\n",
      "[batch    90, sample  1440] loss: 0.324\n",
      "[batch    99, sample  1584] loss: 0.317\n",
      "=== Starting epoch 5 ===\n",
      "[batch     9, sample   144] loss: 0.357\n",
      "[batch    18, sample   288] loss: 0.338\n",
      "[batch    27, sample   432] loss: 0.321\n",
      "[batch    36, sample   576] loss: 0.314\n",
      "[batch    45, sample   720] loss: 0.317\n",
      "[batch    54, sample   864] loss: 0.311\n",
      "[batch    63, sample  1008] loss: 0.308\n",
      "[batch    72, sample  1152] loss: 0.323\n",
      "[batch    81, sample  1296] loss: 0.315\n",
      "[batch    90, sample  1440] loss: 0.321\n",
      "[batch    99, sample  1584] loss: 0.314\n",
      "=== Starting epoch 6 ===\n",
      "[batch     9, sample   144] loss: 0.354\n",
      "[batch    18, sample   288] loss: 0.334\n",
      "[batch    27, sample   432] loss: 0.318\n",
      "[batch    36, sample   576] loss: 0.313\n",
      "[batch    45, sample   720] loss: 0.315\n",
      "[batch    54, sample   864] loss: 0.310\n",
      "[batch    63, sample  1008] loss: 0.306\n",
      "[batch    72, sample  1152] loss: 0.320\n",
      "[batch    81, sample  1296] loss: 0.312\n",
      "[batch    90, sample  1440] loss: 0.319\n",
      "[batch    99, sample  1584] loss: 0.312\n",
      "=== Starting epoch 7 ===\n",
      "[batch     9, sample   144] loss: 0.353\n",
      "[batch    18, sample   288] loss: 0.331\n",
      "[batch    27, sample   432] loss: 0.316\n",
      "[batch    36, sample   576] loss: 0.311\n",
      "[batch    45, sample   720] loss: 0.314\n",
      "[batch    54, sample   864] loss: 0.308\n",
      "[batch    63, sample  1008] loss: 0.305\n",
      "[batch    72, sample  1152] loss: 0.319\n",
      "[batch    81, sample  1296] loss: 0.311\n",
      "[batch    90, sample  1440] loss: 0.317\n",
      "[batch    99, sample  1584] loss: 0.310\n",
      "=== Starting epoch 8 ===\n",
      "[batch     9, sample   144] loss: 0.350\n",
      "[batch    18, sample   288] loss: 0.329\n",
      "[batch    27, sample   432] loss: 0.313\n",
      "[batch    36, sample   576] loss: 0.310\n",
      "[batch    45, sample   720] loss: 0.312\n",
      "[batch    54, sample   864] loss: 0.306\n",
      "[batch    63, sample  1008] loss: 0.303\n",
      "[batch    72, sample  1152] loss: 0.317\n",
      "[batch    81, sample  1296] loss: 0.309\n",
      "[batch    90, sample  1440] loss: 0.317\n",
      "[batch    99, sample  1584] loss: 0.309\n",
      "=== Starting epoch 9 ===\n",
      "[batch     9, sample   144] loss: 0.350\n",
      "[batch    18, sample   288] loss: 0.326\n",
      "[batch    27, sample   432] loss: 0.310\n",
      "[batch    36, sample   576] loss: 0.310\n",
      "[batch    45, sample   720] loss: 0.311\n",
      "[batch    54, sample   864] loss: 0.306\n",
      "[batch    63, sample  1008] loss: 0.302\n",
      "[batch    72, sample  1152] loss: 0.315\n",
      "[batch    81, sample  1296] loss: 0.307\n",
      "[batch    90, sample  1440] loss: 0.315\n",
      "[batch    99, sample  1584] loss: 0.308\n",
      "=== Starting epoch 10 ===\n",
      "[batch     9, sample   144] loss: 0.349\n",
      "[batch    18, sample   288] loss: 0.324\n",
      "[batch    27, sample   432] loss: 0.309\n",
      "[batch    36, sample   576] loss: 0.308\n",
      "[batch    45, sample   720] loss: 0.310\n",
      "[batch    54, sample   864] loss: 0.305\n",
      "[batch    63, sample  1008] loss: 0.301\n",
      "[batch    72, sample  1152] loss: 0.315\n",
      "[batch    81, sample  1296] loss: 0.307\n",
      "[batch    90, sample  1440] loss: 0.314\n",
      "[batch    99, sample  1584] loss: 0.307\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SIGN model, optimizer and loss function\n",
    "model = GNN_expand(hidden_channels=64, n_edge_features = Adj.shape[-1])\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Adjust learning rate\n",
    "criterion = MCRMSE # Mean column-wise root mean square error (MCRMSE) loss\n",
    "\n",
    "# Define trainer function for GNN\n",
    "def run_training(X_data, Adj_data, batch_size = 1, n_epochs = 1):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"=== Starting epoch {epoch + 1} ===\")\n",
    "        # Get permutation of sample indices for shuffling\n",
    "        permutation = torch.randperm(len(X_data))\n",
    "        # permutation = range(len(X_data))\n",
    "        # Define variable for printing training loss\n",
    "        running_loss = 0.\n",
    "        # Run training over mini-batches for current epoch\n",
    "        for ix in range(0, len(X_data), batch_size):  # Iterate over samples in the training dataset\n",
    "            batch_indices = permutation[ix:ix+batch_size] # Get shuffled indices for minibatch\n",
    "            X_batch, Adj_batch = X_data[batch_indices], Adj_data[batch_indices] # Minibatch of X and Adj\n",
    "            y_batch = y_train[batch_indices] # Minibatch of ground truths\n",
    "            out = model(X_batch, Adj_batch) # Perform forward pass\n",
    "            loss = criterion(torch.tensor(y_batch), out)  # Compute the loss\n",
    "            # As the loss function is defined per sample, we have to reduce the loss for\n",
    "            # each mini-batch to a singular value in some way.\n",
    "            # Could for example use mean, sum, or random sample. This is a design choice.\n",
    "            loss = torch.mean(loss) # Calculate average loss for minibatch\n",
    "            loss.backward()  # Derive gradients\n",
    "            optimizer.step()  # Update parameters based on gradients\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            # Print statistics every print_batch minibatches \n",
    "            print_batch = int(len(X_data)/10/batch_size) # Set to print every 1/10 of all samples\n",
    "            running_loss += loss.item() # Add (average) loss from minibatch\n",
    "            if int(ix/batch_size) % print_batch == 0 and ix != 0: # Ignore first minibatch\n",
    "                print('[batch %5d, sample %5d] loss: %.3f' % \n",
    "                        (int(ix/batch_size), ix, \n",
    "                        running_loss / print_batch)) # Average running loss\n",
    "                running_loss = 0. # Reset running loss\n",
    "\n",
    "# Convert training data inputs to pytorch tensors and run training\n",
    "X_torch = torch.tensor(X)\n",
    "Adj_torch = torch.tensor(Adj)\n",
    "run_training(X_torch, Adj_torch, batch_size = 16, n_epochs = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def run_prediction(X_data, Adj_data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    outs = model(X_data, Adj_data) # Feed the data through the network \n",
    "    for yx in outs:\n",
    "        y_pred.append(yx.detach().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shapes of inputs - Private test, post deadline\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_postdeadline' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-db9b691093d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Node features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_test_postdeadline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_node_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_postdeadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mX_test_postdeadline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_postdeadline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Convert to floats to prepare for torch model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Node features X: (n_samples, n_nodes, n_node_features) \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_postdeadline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_postdeadline' is not defined"
     ]
    }
   ],
   "source": [
    "# Construct node features and adjacency matrix for post deadline test data\n",
    "print(\"Shapes of inputs - Private test, post deadline\")\n",
    "\n",
    "# Version 3\n",
    "# A includes base pairs \n",
    "# B includes bpps\n",
    "# D_i includes distance measure, with power i (use several i)\n",
    "\n",
    "# Node features\n",
    "X_test_postdeadline = get_node_features(data = test_postdeadline)\n",
    "X_test_postdeadline = X_test_postdeadline.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Node features X: (n_samples, n_nodes, n_node_features) \", X_test_postdeadline.shape)\n",
    "# Structure adjacency \n",
    "Adj_pairs_test_postdeadline = get_struct_adj(data = test_postdeadline, sequential_edges=False)\n",
    "print(\"Structure adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_pairs_test_postdeadline.shape)\n",
    "# Base pair probability adjacency\n",
    "print(\"Base pair probability adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_bpps_test_postdeadline.shape)\n",
    "# Distance adjacency\n",
    "Adj_dist_1_pd = get_dist_adj(data = test_postdeadline, power = 1)\n",
    "Adj_dist_2_pd = get_dist_adj(data = test_postdeadline, power = 2)\n",
    "Adj_dist_3_pd = get_dist_adj(data = test_postdeadline, power = 3)\n",
    "Adj_dist_test_postdeadline = np.concatenate([Adj_dist_1_pd[:,:,None], Adj_dist_2_pd[:,:,None], Adj_dist_3_pd[:,:,None]], axis = 2)\n",
    "Adj_dist_test_postdeadline = Adj_dist_test_postdeadline[None, :,:,:] # Expand the dimensions of the array to allow stacking matrices for all samples \n",
    "Adj_dist_test_postdeadline = np.repeat(Adj_dist_test_postdeadline, len(test_postdeadline), axis = 0) # Repeat the distance array for each sample (they are identical, simply to match the data shape)\n",
    "print(\"Distance adjacency matrices: (n_samples, n_nodes, n_nodes) \", Adj_dist_test_postdeadline.shape)\n",
    "# Concatenate adjacency matrices into one array along last dimension\n",
    "Adj_test_postdeadline = np.concatenate([Adj_pairs_test_postdeadline[:,:,:,None], Adj_bpps_test_postdeadline[:,:,:,None], Adj_dist_test_postdeadline[:,:,:,:]], axis = 3) # Expand dimensions of adjacency matrices and stack along new dimension\n",
    "Adj_test_postdeadline = Adj_test_postdeadline.astype(np.float32) # Convert to floats to prepare for torch model\n",
    "print(\"Total adjacency matrix: (n_samples, n_nodes, n_nodes, n_edge_features) \", Adj_test_postdeadline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on training data, all 5 targets: 0.3076\nMean score on training data, only scored targets: 0.31642\n"
     ]
    }
   ],
   "source": [
    "# Run prediction on training data as a test run\n",
    "y_train_pred = run_prediction(X_torch, Adj_torch)\n",
    "y_train_pred = y_train_pred.astype(np.float32)\n",
    "\n",
    "# Calculate score on training data\n",
    "y_train_torch = torch.tensor(y_train)\n",
    "y_train_pred_torch = torch.tensor(y_train_pred)\n",
    "training_score = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=False)\n",
    "training_score_only_scored = MCRMSE(y_train_torch, y_train_pred_torch, only_scored=True)\n",
    "\n",
    "print(f\"Mean score on training data, all 5 targets: {float(torch.mean(training_score)):.5}\")\n",
    "print(f\"Mean score on training data, only scored targets: {float(torch.mean(training_score_only_scored)):.5}\")\n",
    "\n",
    "# Training score\n",
    "# SIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean score on private test data, only scored targets: 0.41776\nNumber of trainable model parameters:  29573\n"
     ]
    }
   ],
   "source": [
    "X_test_postdeadline_torch = torch.tensor(X_test_postdeadline)\n",
    "Adj_test_postdeadline_torch = torch.tensor(Adj_test_postdeadline)\n",
    "y_postdeadline_pred = run_prediction(X_test_postdeadline_torch, Adj_test_postdeadline_torch)\n",
    "y_postdeadline_pred = y_postdeadline_pred.astype(np.float32)\n",
    "# Post deadline data only includes the scored targets. Remove the two unscored targets from predictions when calculating score\n",
    "y_postdeadline_pred =  y_postdeadline_pred[:,:,:3]# Remove deg_pH10 and deg_50C (unscored targets, to match post deadline test data)\n",
    "\n",
    "# Calculate score on post deadline test data\n",
    "y_test_postdeadline_torch = torch.tensor(y_test_postdeadline)\n",
    "y_postdeadline_pred_torch = torch.tensor(y_postdeadline_pred)\n",
    "postdeadline_score_only_scored = MCRMSE(y_test_postdeadline_torch, y_postdeadline_pred_torch, only_scored=True, data=test_postdeadline)\n",
    "\n",
    "print(f\"Mean score on private test data, only scored targets: {float(torch.mean(postdeadline_score_only_scored)):.5}\")\n",
    "\n",
    "# Print number of trainable parameters\n",
    "print(\"Number of trainable model parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "# Test score\n",
    "# SIGN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ce1e901da04acbe50dd99cd0d91cb411cb2166c3507dec81ce620981a8df4741"
   }
  },
  "interpreter": {
   "hash": "ce1e901da04acbe50dd99cd0d91cb411cb2166c3507dec81ce620981a8df4741"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}